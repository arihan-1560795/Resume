{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Test writing, reading csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DF in memory matches DF written to disk: True\n"
     ]
    }
   ],
   "source": [
    "#Create toy dataframe with numbers, strings, missing values\n",
    "toyNum = [1,2,3,4,5]\n",
    "toyChar = ['a','b','c','d','e']\n",
    "toyNull = [np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "toyDf = pd.DataFrame.from_dict({'num': toyNum, 'string': toyChar, 'null': toyNull})\n",
    "\n",
    "#Write to local csv file\n",
    "toyDf.to_csv('toyDf.csv', sep='\\t', encoding='utf-8', index=False)\n",
    "\n",
    "#Read local csv file\n",
    "toyDf2 = pd.read_csv(\"C:\\\\Users\\\\Arihan Jalan\\\\INFO 371\\\\toyDf.csv\", sep='\\t')\n",
    "\n",
    "#Print whether created, imported df are the same\n",
    "print(\"Created DF in memory matches DF written to disk:\", toyDf.equals(toyDf2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load rotten tomatoes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Rotten Tomatoes dataset\n",
    "df = pd.read_csv(\"D:\\\\OneDrive\\\\University of Washington Seattle\\\\Year 4 Quarter 2 (Winter)\\\\INFO 371\\\\Problem Set\\\\PS6\\\\rotten-tomatoes.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Split df into train, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 10753\n",
      "Size of training relative to full dataset: 0.8\n"
     ]
    }
   ],
   "source": [
    "#Split dataset into 80-20 train-test\n",
    "df_train, df_test = train_test_split(df, train_size = 0.8, test_size = 0.2, random_state = 1)\n",
    "\n",
    "#Ensure train size looks alright\n",
    "print('Size of training set:', len(df_train))\n",
    "print('Size of training relative to full dataset:', np.round(len(df_train) / len(df), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Store test on drive; delete from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write df_test to storage; note- stored within C:\\\\Users\\\\Arihan Jalan\\\\INFO 371\\\\\n",
    "df_test.to_csv('rotten-tomatoes-test.csv', sep='\\t', encoding='utf-8', index=False)\n",
    "\n",
    "#Delete from memory\n",
    "del df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Explore and clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>link</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9924</th>\n",
       "      <td>Peter Rainer</td>\n",
       "      <td>rotten</td>\n",
       "      <td>120633</td>\n",
       "      <td>http://www.newyorkmetro.com/nymetro/movies/rev...</td>\n",
       "      <td>New York Magazine</td>\n",
       "      <td>I'm all for films that don't flow from the usu...</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>10500</td>\n",
       "      <td>A Civil Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873</th>\n",
       "      <td>Bosley Crowther</td>\n",
       "      <td>fresh</td>\n",
       "      <td>61809</td>\n",
       "      <td>http://movies2.nytimes.com/mem/movies/review.h...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Excellent quasidocumentary, which sends shiver...</td>\n",
       "      <td>2003-05-20 00:00:00</td>\n",
       "      <td>12934</td>\n",
       "      <td>In Cold Blood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4979</th>\n",
       "      <td>Amy Taubin</td>\n",
       "      <td>fresh</td>\n",
       "      <td>89881</td>\n",
       "      <td>http://www.villagevoice.com/issues/0033/taubin...</td>\n",
       "      <td>Village Voice</td>\n",
       "      <td>For aficionados of the war movie, the western,...</td>\n",
       "      <td>2008-07-01 00:00:00</td>\n",
       "      <td>15203</td>\n",
       "      <td>Ran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2858</th>\n",
       "      <td>Jonathan Rosenbaum</td>\n",
       "      <td>fresh</td>\n",
       "      <td>117608</td>\n",
       "      <td>http://onfilm.chicagoreader.com/movies/capsule...</td>\n",
       "      <td>Chicago Reader</td>\n",
       "      <td>[A] celebration of high jinks.</td>\n",
       "      <td>2008-05-19 00:00:00</td>\n",
       "      <td>10970</td>\n",
       "      <td>Sgt. Bilko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4615</th>\n",
       "      <td>Teresa Wiltz</td>\n",
       "      <td>rotten</td>\n",
       "      <td>362269</td>\n",
       "      <td>http://www.washingtonpost.com/wp-dyn/articles/...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>There's a flatness to the filmmaking; what's n...</td>\n",
       "      <td>2004-11-19 00:00:00</td>\n",
       "      <td>13114</td>\n",
       "      <td>Kinsey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  critic   fresh    imdb  \\\n",
       "9924        Peter Rainer  rotten  120633   \n",
       "1873     Bosley Crowther   fresh   61809   \n",
       "4979          Amy Taubin   fresh   89881   \n",
       "2858  Jonathan Rosenbaum   fresh  117608   \n",
       "4615        Teresa Wiltz  rotten  362269   \n",
       "\n",
       "                                                   link        publication  \\\n",
       "9924  http://www.newyorkmetro.com/nymetro/movies/rev...  New York Magazine   \n",
       "1873  http://movies2.nytimes.com/mem/movies/review.h...     New York Times   \n",
       "4979  http://www.villagevoice.com/issues/0033/taubin...      Village Voice   \n",
       "2858  http://onfilm.chicagoreader.com/movies/capsule...     Chicago Reader   \n",
       "4615  http://www.washingtonpost.com/wp-dyn/articles/...    Washington Post   \n",
       "\n",
       "                                                  quote          review_date  \\\n",
       "9924  I'm all for films that don't flow from the usu...  2000-01-01 00:00:00   \n",
       "1873  Excellent quasidocumentary, which sends shiver...  2003-05-20 00:00:00   \n",
       "4979  For aficionados of the war movie, the western,...  2008-07-01 00:00:00   \n",
       "2858                     [A] celebration of high jinks.  2008-05-19 00:00:00   \n",
       "4615  There's a flatness to the filmmaking; what's n...  2004-11-19 00:00:00   \n",
       "\n",
       "       rtid           title  \n",
       "9924  10500  A Civil Action  \n",
       "1873  12934   In Cold Blood  \n",
       "4979  15203             Ran  \n",
       "2858  10970      Sgt. Bilko  \n",
       "4615  13114          Kinsey  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Return 5 random rows from training data\n",
    "df_train.sample(n=5, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Print variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable names:\n",
      "  * critic\n",
      "  * fresh\n",
      "  * imdb\n",
      "  * link\n",
      "  * publication\n",
      "  * quote\n",
      "  * review_date\n",
      "  * rtid\n",
      "  * title\n"
     ]
    }
   ],
   "source": [
    "#Prints column header names\n",
    "\n",
    "#Extract array of column names\n",
    "df_train_cols = df_train.columns\n",
    "\n",
    "print(\"Variable names:\")\n",
    "\n",
    "#Iterate over them, printing them out\n",
    "for name in df_train_cols:\n",
    "    print(\"  *\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Summary Table\n",
    "Note- writeup says \"maybe more like a bullet list\". A table following the instructions will have some cols having NaN values for certain rows; so I'm instead having a bullet list for this section as suggested by the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.a. Number of missing in fresh, quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values for 'fresh' column: 0\n",
      "Number of missing values for 'quote' column: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of missing values for 'fresh' column:\", df_train['fresh'].isnull().sum())\n",
    "print(\"Number of missing values for 'quote' column:\", df_train['quote'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.b. Different possible values for fresh/rotten evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible values within 'fresh' column:\n",
      "    rotten\n",
      "    fresh\n",
      "    none\n"
     ]
    }
   ],
   "source": [
    "#Extract Unique values for fresh column\n",
    "arr_unique_fresh = df_train.fresh.unique()\n",
    "\n",
    "#Iterate over array of unique values, printing them\n",
    "print(\"Possible values within 'fresh' column:\")\n",
    "for name in arr_unique_fresh:\n",
    "    print('   ', name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.c. Counts for fresh / rotten evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fresh</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fresh</td>\n",
       "      <td>6733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>none</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rotten</td>\n",
       "      <td>4003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fresh  counts\n",
       "0   fresh    6733\n",
       "1    none      17\n",
       "2  rotten    4003"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note- ...fresh').count() will return counts per col (how many cols correspond to fresh, none, rotten)\n",
    "#df_train.groupby('fresh').size()   group by 'fresh' column; calculate size\n",
    "#...).reset_index(name='counts')    reset index; add col. convert to dataframe\n",
    "df_train.groupby('fresh').size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.d. Number of zero-length/whitespace quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values in 'quote' column with empty string (zero length): 0\n",
      "Number of values in 'quote' column with white spaces: 0\n"
     ]
    }
   ],
   "source": [
    "#Create an array of values from quote column\n",
    "arr_quote = df_train.quote.values.copy()\n",
    "\n",
    "#Print number of quotes that are empty strings\n",
    "print(\"Number of values in 'quote' column with empty string (zero length):\", sum(arr_quote == ''))\n",
    "\n",
    "#Remove white spaces by iterating through array\n",
    "#AFTER- array has no whitespaces\n",
    "for i in range(len(arr_quote)):\n",
    "    arr_quote[i] = \"\".join(arr_quote[i].split())\n",
    "\n",
    "#Print quotes that have whitespaces\n",
    "print(\"Number of values in 'quote' column with white spaces:\", sum(df_train.quote.values == \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.e. min-max-avg length of quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quote length in charachters\n",
      "\n",
      "Smallest quote length: 4\n",
      "Largest quote length:  256\n",
      "Average quote length:  121.133\n"
     ]
    }
   ],
   "source": [
    "#Create an array of values from quote column\n",
    "arr_quote = df_train.quote.values.copy()\n",
    "\n",
    "#Create an array of quote string length\n",
    "arr_quote_len = []\n",
    "for i in range(len(arr_quote)):\n",
    "    arr_quote_len.append(len(arr_quote[i]))\n",
    "    \n",
    "#Print min, max, avg length\n",
    "print(\"Quote length in charachters\")\n",
    "print(\"\")\n",
    "print(\"Smallest quote length:\", min(arr_quote_len))\n",
    "print(\"Largest quote length: \", max(arr_quote_len))\n",
    "print(\"Average quote length: \", np.round(np.mean(arr_quote_len), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.f. How many reviews are in the data multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows for data in 'quote', 'title', 'critic': 403\n"
     ]
    }
   ],
   "source": [
    "#Duplicate rows for same quote, title, critic value\n",
    "val_duplicate = len(df_train[df_train.duplicated(subset=['quote', 'title', 'critic'])])\n",
    "print(\"Number of duplicate rows for data in 'quote', 'title', 'critic':\", val_duplicate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes dataframe of movie reviews\n",
    "\n",
    "#Returns copy of dataframe with\n",
    "# * reset indexes, \n",
    "# * lowercase quotes, \n",
    "# * whitespaces in quotes replaced with single space, \n",
    "\n",
    "#Also, removes rows where-\n",
    "# * 'fresh' = none\n",
    "# * 'quote' = '' or is None\n",
    "# * duplicate values\n",
    "def cleanReviews(df):\n",
    "\n",
    "    #Makes copy of df so changes don't effect sent one\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    #BELOW CODE REMOVES FRESH=NONE AND QUOTE=EMPTY\n",
    "    \n",
    "    #Filter operation on data\n",
    "    df_copy = df_copy[(df_copy.fresh != 'none') & (df_copy.quote != '') & (df_copy.quote != None)]\n",
    "\n",
    "    #BELOW CODE REMOVES DUPLICATES\n",
    "    \n",
    "    #Removing duplicates by grouping by all columns\n",
    "    df_copy = pd.DataFrame(df_copy.groupby(list(df_copy)).size().reset_index())\n",
    "\n",
    "    #BELOW CODE CLEANS QUOTES\n",
    "\n",
    "    #Cleaning data by removing punctuation, numbers, etc. from quotes\n",
    "    df_copy.quote = df_copy.quote.str.lower()\n",
    "    \n",
    "    #Create an array of values from quote column\n",
    "    arr_quote = df_copy.quote.values\n",
    "    \n",
    "    for i in range(len(arr_quote)):\n",
    "        #Remove whitespaces\n",
    "        arr_quote[i] = \" \".join(arr_quote[i].split())\n",
    "\n",
    "        #Remove punctuation\n",
    "        arr_quote[i] = arr_quote[i].translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "    df_copy.quote = arr_quote\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def addFreshClass(df):\n",
    "    Fresh = {'fresh': 1,'rotten': 0} \n",
    "\n",
    "    #Loop dataframe and write value where key matches\n",
    "    fresh_yes = [Fresh[item] for item in df.fresh]\n",
    "\n",
    "    #Append cancer diagnosis column\n",
    "    df['fresh_yes'] = fresh_yes\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean data\n",
    "df_train = addFreshClass(cleanReviews(df_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Ensure Familiar with Naive Bayes (Done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Convert data into BOW (done through createBOWdataframe)\n",
    "Note- instead of using a BOW; handling it in a different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE DF BOW\n",
    "#Returns dataframe with columns-\n",
    "# * word- the word token (e.g.- \"it\", \"has\", \"the\", \"of\")\n",
    "# * word_rotten- the number of reviews/quotes labelled as rotten that contain the given word\n",
    "# * word_fresh- the number of reviews/quotes labelled as fresh that contain the given word\n",
    "# * p_word_fresh- given a quote labelled as fresh, the probability it will contain the given word\n",
    "# * p_word_rotten- given a quote labelled as rotten, the probability it will contain the given word\n",
    "# * p_log_word_fresh- log(p_word_fresh)\n",
    "# * p_log_word_rotten- log(p_word_rotten)\n",
    "def createBOWdataframe(alpha, df_input, cnt_fresh, cnt_rotten):\n",
    "\n",
    "    #bowdf Dataframe with word, count in spam, count in ham\n",
    "    arr_word = []\n",
    "    arr_word_rotten = []\n",
    "    arr_word_fresh = []\n",
    "    alpha = alpha\n",
    "\n",
    "    #for every row in the dataframe\n",
    "    for i in range(len(df_input)):\n",
    "\n",
    "    #   Split sentence into array of words\n",
    "        arr_word_tokens = df_input.quote.values[i].split()\n",
    "\n",
    "    #   empty array of seen words; stores words we've already seen\n",
    "        arr_word_tokens_seen = []\n",
    "\n",
    "    #   is sentence fresh?\n",
    "        bool_is_fresh = df_input.fresh.values[i] == 'fresh'\n",
    "\n",
    "    #   for each word in the sentence\n",
    "        for word_token in arr_word_tokens:\n",
    "\n",
    "    #   if not in array of seen words\n",
    "    #   if we have seen the word in the sentence below, don't enter\n",
    "            if word_token not in arr_word_tokens_seen:\n",
    "\n",
    "    #           WORD_TOKEN NOT SEEN BEFORE IN SENTENCE\n",
    "    #           Note- we don't want to consider the same word twice for a given review- NB considers T/F, not count for a review\n",
    "\n",
    "    #           add to array of seen words\n",
    "                arr_word_tokens_seen.append(word_token)\n",
    "\n",
    "    #           WORD NOT IN BOWDF; ADD WORD TO BOWDF\n",
    "    #           if the word isn't in bowdf; create an entry for it\n",
    "                if word_token not in arr_word:\n",
    "\n",
    "\n",
    "    #               WORD_TOKEN NOT SEEN BEFORE IN ARR WORDS\n",
    "    #               Note- IT'S A NEW WORD; SO WE ADD IT TO OUR BOWDF\n",
    "    #               Note- DON'T FORGET TO ROW IN COUNT ARRAY TOO!\n",
    "\n",
    "    #               add to bowdf, with initial value of alpha\n",
    "                    arr_word.append(word_token)\n",
    "                    arr_word_rotten.append(alpha)\n",
    "                    arr_word_fresh.append(alpha)\n",
    "\n",
    "    #           WORD IN BOWDF; INCREMENT COUNT\n",
    "\n",
    "    #           if fresh, increment bowdf[location].fresh\n",
    "                if bool_is_fresh:\n",
    "                    arr_word_fresh[arr_word.index(word_token)] += 1.0\n",
    "\n",
    "    #           else, increment rotten\n",
    "                else:\n",
    "                    arr_word_rotten[arr_word.index(word_token)] += 1.0\n",
    "\n",
    "\n",
    "    df_output = pd.DataFrame.from_dict({'word': arr_word, 'word_rotten': arr_word_rotten, 'word_fresh': arr_word_fresh})\n",
    "\n",
    "    #Add col for P(w|Fresh) = count(word)/count(Fresh)\n",
    "    df_output['p_word_fresh'] = df_output['word_fresh'] / cnt_fresh\n",
    "\n",
    "    #Add col for P(w|Rotten) = count(word)/count(Rotten)\n",
    "    df_output['p_word_rotten'] = df_output['word_rotten'] / cnt_rotten\n",
    "\n",
    "    #Add col for P(w|Fresh) = count(word)/count(Fresh)\n",
    "    df_output['p_log_word_fresh'] = ma.log(df_output['p_word_fresh'].values).filled(0)\n",
    "\n",
    "    #Add col for P(w|Rotten) = count(word)/count(Rotten)\n",
    "    df_output['p_log_word_rotten'] = ma.log(df_output['p_word_rotten'].values).filled(0)\n",
    "    \n",
    "    return df_output\n",
    "\n",
    "\n",
    "#RETURNS SUM OF LOG PROBABILITIES FOR TOKEN SET\n",
    "#Input- \n",
    "# * array of tokenized words \n",
    "# * array of probabilities for above tokenized words\n",
    "# * log(prob(fresh))- the log probability for a given class given by n_class/n_total\n",
    "# * array of tokenized words that need to be classified \n",
    "#Note- ignores tokens not in the array of tokenized words\n",
    "#      returnes log prob calculated by- log(prob(class)) + sum( log(prob(word needing classification)) )\n",
    "def logProbabilityForFresh(arr_word, arr_prob, p_log_class, arr_word_tokens):\n",
    "   \n",
    "    #empty array of prob\n",
    "    arr_word_prob = []\n",
    "\n",
    "    #empty array of seen words\n",
    "    arr_word_tokens_seen = []\n",
    "    \n",
    "    #for each word\n",
    "    for word_token in arr_word_tokens:\n",
    "\n",
    "    #   Is the word in the bowdf\n",
    "        if word_token in arr_word:\n",
    "            \n",
    "            index_word = arr_word.index(word_token)\n",
    "\n",
    "    #       if not in array of seen words\n",
    "            if word_token not in arr_word_tokens_seen:\n",
    "\n",
    "    #           add to array of seen words\n",
    "                arr_word_tokens_seen.append(word_token)\n",
    "\n",
    "    #           add prob\n",
    "                arr_word_prob.append(arr_prob[index_word])\n",
    "\n",
    "    return p_log_class + sum(arr_word_prob)\n",
    "\n",
    "#Input-   \n",
    "# * sentence that needs to be classified\n",
    "# * dataframe representing bag-of-words\n",
    "# * log(prob(fresh))- the log probability for fresh given by n_class/n_total\n",
    "# * log(prob(rotten))- the log probability for rotten given by n_class/n_total\n",
    "#Returns prediction number < 1 if fresh; > 1 if rotten\n",
    "def evalSentence(sentence, df_word, p_log_fresh, p_log_rotten):    \n",
    "\n",
    "    #words in sentence tokenized\n",
    "    arr_word_tokens = sentence.split()\n",
    "\n",
    "    #BOWDF wordlist\n",
    "    arr_word = df_word['word'].values.tolist()\n",
    "\n",
    "    #compute log fresh\n",
    "    arr_word_prob_fresh = df_word['p_log_word_fresh'].values.tolist()\n",
    "    p_test_fresh = logProbabilityForFresh(arr_word, arr_word_prob_fresh, p_log_fresh, arr_word_tokens)\n",
    "\n",
    "    #compute log rotten\n",
    "    arr_word_prob_rotten = df_word['p_log_word_rotten'].values.tolist()\n",
    "    p_test_rotten = logProbabilityForFresh(arr_word, arr_word_prob_rotten, p_log_rotten, arr_word_tokens)\n",
    "\n",
    "    #if p_test_fresh = -0.5 and rotten = -1 (fresh > rotten)\n",
    "    #fresh/rotten < 1\n",
    "    return p_test_fresh / p_test_rotten\n",
    "\n",
    "\n",
    "\n",
    "#Takes alpha value (smoothing), training data, testing data; boolean for returning log likihood instead of prediction.\n",
    "#returns array of predicted values for testing\n",
    "def myNBimplementation(alpha, input_df_train, input_df_test, return_raw_p=True):\n",
    "\n",
    "    #=========================================================================================\n",
    "    #=========================================================================================\n",
    "    #=========================================================================================\n",
    "\n",
    "    #COMPUTE PROBABILITY FOR FRESH, ROTTEN COUNTS\n",
    "    #count of fresh and rotten in training\n",
    "    cnt_fresh = list(input_df_train['fresh'].values.flatten()).count('fresh') + alpha\n",
    "    cnt_rotten = list(input_df_train['fresh'].values.flatten()).count('rotten') + alpha\n",
    "\n",
    "    #p(fresh)\n",
    "    p_fresh = cnt_fresh / (cnt_fresh + cnt_rotten)\n",
    "\n",
    "    #p(rotten)\n",
    "    p_rotten = cnt_rotten / (cnt_fresh + cnt_rotten)\n",
    "\n",
    "    #p(fresh)\n",
    "    p_log_fresh = np.log(p_fresh)\n",
    "\n",
    "    #p(rotten)\n",
    "    p_log_rotten = np.log(p_rotten)\n",
    "    \n",
    "    #=========================================================================================\n",
    "    #=========================================================================================\n",
    "    #=========================================================================================\n",
    "\n",
    "    #CREATE BOWDF\n",
    "    #Creates dataframe that is BOW for words in the training set\n",
    "    df_word = createBOWdataframe(alpha, input_df_train, cnt_fresh, cnt_rotten)\n",
    "\n",
    "    #=========================================================================================\n",
    "    #=========================================================================================\n",
    "    #=========================================================================================\n",
    "\n",
    "    #COMPUTE PREDICTIONS\n",
    "\n",
    "    #Create array to store actual, predicted values\n",
    "    arr_fresh_pred = []\n",
    "\n",
    "    #list of testing sentences\n",
    "    arr_sentences = input_df_test.quote.values\n",
    "\n",
    "    #for each sentence\n",
    "    for sentence in arr_sentences:\n",
    "\n",
    "        #calculate probability of fresh\n",
    "        p_class = evalSentence(sentence, df_word, p_log_fresh, p_log_rotten)\n",
    "\n",
    "        #don't assign 1/0 prediction if return_raw_p=False\n",
    "        #instead, append the raw p_class\n",
    "        if return_raw_p:\n",
    "\n",
    "            #put prediction into array\n",
    "            if(1 > p_class):\n",
    "                arr_fresh_pred.append(1)\n",
    "            else:\n",
    "                arr_fresh_pred.append(0)\n",
    "                \n",
    "        else:\n",
    "            arr_fresh_pred.append(p_class)\n",
    "\n",
    "    return arr_fresh_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Split data into training/validation blocks\n",
    "Note- calculating BOW-DF here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_V, df_test_V = train_test_split(df_train, train_size = 0.8, test_size = 0.2, random_state = 1)\n",
    "df_train_V = df_train_V.reset_index(drop=True)\n",
    "df_test_V = df_test_V.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Log probability tomato is fresh/rotten p(F), p(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done within myNBimplementation above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Calculate p(w|F), p(w|R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computed within createBOWdataframe function above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Calculate class log-liklihood for validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computed within myNBimplementation above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Print confusion matrix, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(actual, pred):\n",
    "    K = len(np.unique(actual)) # Number of classes \n",
    "    result = np.zeros((K, K), dtype='int')\n",
    "    for i in range(len(actual)):\n",
    "        result[actual[i]][pred[i]] += 1\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.4455799693408278\n",
      "Confusion Matrix: \n",
      " [[470 270]\n",
      " [815 402]]\n"
     ]
    }
   ],
   "source": [
    "arr_fresh_pred = myNBimplementation(0.0, df_train_V, df_test_V)\n",
    "arr_fresh_true = df_test_V.fresh_yes\n",
    "\n",
    "print(\"accuracy:\", np.mean(arr_fresh_pred == arr_fresh_true))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(arr_fresh_true, arr_fresh_pred))\n",
    "\n",
    "del arr_fresh_pred\n",
    "del arr_fresh_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Consider only words appearing > 30 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_fresh = list(df_train_V['fresh'].values.flatten()).count('fresh')\n",
    "cnt_rotten = list(df_train_V['fresh'].values.flatten()).count('rotten')\n",
    "df_word = createBOWdataframe(0, df_train_V, cnt_fresh, cnt_rotten)\n",
    "\n",
    "df_word['word_count'] = df_word.word_rotten + df_word.word_fresh\n",
    "df_word = df_word[(df_word.word_count > 30)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 10 Best words to predict rotten/fresh status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>word_rotten</th>\n",
       "      <th>word_fresh</th>\n",
       "      <th>p_word_fresh</th>\n",
       "      <th>p_word_rotten</th>\n",
       "      <th>p_log_word_fresh</th>\n",
       "      <th>p_log_word_rotten</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the</td>\n",
       "      <td>1858.0</td>\n",
       "      <td>3080.0</td>\n",
       "      <td>0.631148</td>\n",
       "      <td>0.630472</td>\n",
       "      <td>-0.460216</td>\n",
       "      <td>-0.461287</td>\n",
       "      <td>4938.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>1324.0</td>\n",
       "      <td>2609.0</td>\n",
       "      <td>0.534631</td>\n",
       "      <td>0.449270</td>\n",
       "      <td>-0.626178</td>\n",
       "      <td>-0.800130</td>\n",
       "      <td>3933.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>a</td>\n",
       "      <td>1450.0</td>\n",
       "      <td>2572.0</td>\n",
       "      <td>0.527049</td>\n",
       "      <td>0.492026</td>\n",
       "      <td>-0.640461</td>\n",
       "      <td>-0.709224</td>\n",
       "      <td>4022.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>of</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>2446.0</td>\n",
       "      <td>0.501230</td>\n",
       "      <td>0.456396</td>\n",
       "      <td>-0.690691</td>\n",
       "      <td>-0.784394</td>\n",
       "      <td>3791.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>is</td>\n",
       "      <td>979.0</td>\n",
       "      <td>1696.0</td>\n",
       "      <td>0.347541</td>\n",
       "      <td>0.332202</td>\n",
       "      <td>-1.056873</td>\n",
       "      <td>-1.102011</td>\n",
       "      <td>2675.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>to</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>0.295492</td>\n",
       "      <td>0.373940</td>\n",
       "      <td>-1.219114</td>\n",
       "      <td>-0.983661</td>\n",
       "      <td>2544.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in</td>\n",
       "      <td>709.0</td>\n",
       "      <td>1197.0</td>\n",
       "      <td>0.245287</td>\n",
       "      <td>0.240584</td>\n",
       "      <td>-1.405327</td>\n",
       "      <td>-1.424687</td>\n",
       "      <td>1906.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>its</td>\n",
       "      <td>618.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.207582</td>\n",
       "      <td>0.209705</td>\n",
       "      <td>-1.572229</td>\n",
       "      <td>-1.562055</td>\n",
       "      <td>1631.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>that</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1010.0</td>\n",
       "      <td>0.206967</td>\n",
       "      <td>0.203597</td>\n",
       "      <td>-1.575195</td>\n",
       "      <td>-1.591613</td>\n",
       "      <td>1610.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>it</td>\n",
       "      <td>596.0</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>0.205738</td>\n",
       "      <td>0.202240</td>\n",
       "      <td>-1.581153</td>\n",
       "      <td>-1.598302</td>\n",
       "      <td>1600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  word_rotten  word_fresh  p_word_fresh  p_word_rotten  \\\n",
       "6     the       1858.0      3080.0      0.631148       0.630472   \n",
       "0     and       1324.0      2609.0      0.534631       0.449270   \n",
       "19      a       1450.0      2572.0      0.527049       0.492026   \n",
       "11     of       1345.0      2446.0      0.501230       0.456396   \n",
       "36     is        979.0      1696.0      0.347541       0.332202   \n",
       "22     to       1102.0      1442.0      0.295492       0.373940   \n",
       "1      in        709.0      1197.0      0.245287       0.240584   \n",
       "2     its        618.0      1013.0      0.207582       0.209705   \n",
       "105  that        600.0      1010.0      0.206967       0.203597   \n",
       "56     it        596.0      1004.0      0.205738       0.202240   \n",
       "\n",
       "     p_log_word_fresh  p_log_word_rotten  word_count  \n",
       "6           -0.460216          -0.461287      4938.0  \n",
       "0           -0.626178          -0.800130      3933.0  \n",
       "19          -0.640461          -0.709224      4022.0  \n",
       "11          -0.690691          -0.784394      3791.0  \n",
       "36          -1.056873          -1.102011      2675.0  \n",
       "22          -1.219114          -0.983661      2544.0  \n",
       "1           -1.405327          -1.424687      1906.0  \n",
       "2           -1.572229          -1.562055      1631.0  \n",
       "105         -1.575195          -1.591613      1610.0  \n",
       "56          -1.581153          -1.598302      1600.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top words for fresh prediction\n",
    "df_word.nlargest(10, columns=['p_word_fresh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>word_rotten</th>\n",
       "      <th>word_fresh</th>\n",
       "      <th>p_word_fresh</th>\n",
       "      <th>p_word_rotten</th>\n",
       "      <th>p_log_word_fresh</th>\n",
       "      <th>p_log_word_rotten</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the</td>\n",
       "      <td>1858.0</td>\n",
       "      <td>3080.0</td>\n",
       "      <td>0.631148</td>\n",
       "      <td>0.630472</td>\n",
       "      <td>-0.460216</td>\n",
       "      <td>-0.461287</td>\n",
       "      <td>4938.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>a</td>\n",
       "      <td>1450.0</td>\n",
       "      <td>2572.0</td>\n",
       "      <td>0.527049</td>\n",
       "      <td>0.492026</td>\n",
       "      <td>-0.640461</td>\n",
       "      <td>-0.709224</td>\n",
       "      <td>4022.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>of</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>2446.0</td>\n",
       "      <td>0.501230</td>\n",
       "      <td>0.456396</td>\n",
       "      <td>-0.690691</td>\n",
       "      <td>-0.784394</td>\n",
       "      <td>3791.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>1324.0</td>\n",
       "      <td>2609.0</td>\n",
       "      <td>0.534631</td>\n",
       "      <td>0.449270</td>\n",
       "      <td>-0.626178</td>\n",
       "      <td>-0.800130</td>\n",
       "      <td>3933.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>to</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>0.295492</td>\n",
       "      <td>0.373940</td>\n",
       "      <td>-1.219114</td>\n",
       "      <td>-0.983661</td>\n",
       "      <td>2544.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>is</td>\n",
       "      <td>979.0</td>\n",
       "      <td>1696.0</td>\n",
       "      <td>0.347541</td>\n",
       "      <td>0.332202</td>\n",
       "      <td>-1.056873</td>\n",
       "      <td>-1.102011</td>\n",
       "      <td>2675.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in</td>\n",
       "      <td>709.0</td>\n",
       "      <td>1197.0</td>\n",
       "      <td>0.245287</td>\n",
       "      <td>0.240584</td>\n",
       "      <td>-1.405327</td>\n",
       "      <td>-1.424687</td>\n",
       "      <td>1906.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>its</td>\n",
       "      <td>618.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.207582</td>\n",
       "      <td>0.209705</td>\n",
       "      <td>-1.572229</td>\n",
       "      <td>-1.562055</td>\n",
       "      <td>1631.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>that</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1010.0</td>\n",
       "      <td>0.206967</td>\n",
       "      <td>0.203597</td>\n",
       "      <td>-1.575195</td>\n",
       "      <td>-1.591613</td>\n",
       "      <td>1610.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>it</td>\n",
       "      <td>596.0</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>0.205738</td>\n",
       "      <td>0.202240</td>\n",
       "      <td>-1.581153</td>\n",
       "      <td>-1.598302</td>\n",
       "      <td>1600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  word_rotten  word_fresh  p_word_fresh  p_word_rotten  \\\n",
       "6     the       1858.0      3080.0      0.631148       0.630472   \n",
       "19      a       1450.0      2572.0      0.527049       0.492026   \n",
       "11     of       1345.0      2446.0      0.501230       0.456396   \n",
       "0     and       1324.0      2609.0      0.534631       0.449270   \n",
       "22     to       1102.0      1442.0      0.295492       0.373940   \n",
       "36     is        979.0      1696.0      0.347541       0.332202   \n",
       "1      in        709.0      1197.0      0.245287       0.240584   \n",
       "2     its        618.0      1013.0      0.207582       0.209705   \n",
       "105  that        600.0      1010.0      0.206967       0.203597   \n",
       "56     it        596.0      1004.0      0.205738       0.202240   \n",
       "\n",
       "     p_log_word_fresh  p_log_word_rotten  word_count  \n",
       "6           -0.460216          -0.461287      4938.0  \n",
       "19          -0.640461          -0.709224      4022.0  \n",
       "11          -0.690691          -0.784394      3791.0  \n",
       "0           -0.626178          -0.800130      3933.0  \n",
       "22          -1.219114          -0.983661      2544.0  \n",
       "36          -1.056873          -1.102011      2675.0  \n",
       "1           -1.405327          -1.424687      1906.0  \n",
       "2           -1.572229          -1.562055      1631.0  \n",
       "105         -1.575195          -1.591613      1610.0  \n",
       "56          -1.581153          -1.598302      1600.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top words for rotten prediction\n",
    "df_word.nlargest(10, columns=['p_word_rotten'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_word\n",
    "del cnt_rotten\n",
    "del cnt_fresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Print misclassified quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quotes misclassified as fresh with highest probability:\n",
      "   1 .  disappointingly colorless and squeakyclean\n",
      "   2 .  misogynistic claptrap\n",
      "   3 .  hohum\n",
      "   4 .  hohum\n",
      "   5 .  unimaginative\n",
      "   6 .  competently made\n",
      "   7 .  silly pointless and obscenely overproduced\n",
      "   8 .  a trite classic\n",
      "   9 .  surprisingly tepid\n",
      "   10 .  a depressingly vacuous star vehicle\n",
      "\n",
      "Quotes misclassified as rotten with highest probability:\n",
      "   1 .  bracingly chilly\n",
      "   2 .  superlatively wellmade\n",
      "   3 .  a meathead burlesque\n",
      "   4 .  big bold and gloriously sweeping\n",
      "   5 .  there has never been a film quite like kasi lemmons shimmering eves bayou\n",
      "   6 .  buoyantly funny\n",
      "   7 .  its worth seeking out definitely\n",
      "   8 .  krzysztof kieslowskis penetrating hypnotic meditation on liberty and loss\n",
      "   9 .  a glibly entertaining corporate thriller\n",
      "   10 .  the backgrounds are as richly textured and detailed as in any other disney film\n"
     ]
    }
   ],
   "source": [
    "arr_fresh_pred = myNBimplementation(0.0, df_train_V, df_test_V)\n",
    "arr_fresh_true = df_test_V.fresh_yes\n",
    "arr_fresh_prob = myNBimplementation(0.0, df_train_V, df_test_V, False)\n",
    "\n",
    "\n",
    "#Assign properly classified values to prob(1)\n",
    "#This works because the probability ranges from less than 1, to greater than 1 for the 10 most certainly classified as fresh/rotten\n",
    "for i in range(len(arr_fresh_pred)):\n",
    "    if arr_fresh_pred[i] == arr_fresh_true[i]:\n",
    "        arr_fresh_prob[i] = 1\n",
    "\n",
    "#Misclassifier as rotten\n",
    "arr_l_prob_wrong = pd.Series(arr_fresh_prob).nlargest(10).values\n",
    "arr_s_prob_wrong = pd.Series(arr_fresh_prob).nsmallest(10).values\n",
    "\n",
    "#Print incorrectly classified values\n",
    "print(\"Quotes misclassified as fresh with highest probability:\")\n",
    "for i in range(10):\n",
    "    print(\"  \", i+1,\". \",df_test_V.quote.values[arr_fresh_prob.index(arr_s_prob_wrong[i])])\n",
    "\n",
    "print(\"\")\n",
    "    \n",
    "#Print incorrectly classified values\n",
    "print(\"Quotes misclassified as rotten with highest probability:\")\n",
    "for i in range(10):\n",
    "    print(\"  \",i+1,\". \",df_test_V.quote.values[arr_fresh_prob.index(arr_l_prob_wrong[i])])\n",
    "    \n",
    "        \n",
    "del arr_fresh_pred\n",
    "del arr_fresh_true\n",
    "del arr_fresh_prob\n",
    "del arr_l_prob_wrong\n",
    "del arr_s_prob_wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fresh quotes are misclassified as rotten probably because they use words associated with rotten reviews like \"competently\", \"surprisingly\", \"classic\", etc; and vice-versa. The algorithm is using stopwords in its predictions which may be throwing things off- because words like \"a\", \"the\", etc. that don't have a negative/positive meaning are being considered. This is the pit-fall of using NB- we're evaluating each word indendently of eachother wherein in actual English- it's important to consider the context in which the word is being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 NB with smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Create func for fitting NB model, another to predict outcome based on fitted model\n",
    "\n",
    "Already done, function logProbabilityForFresh calculates log probability (NB), and evalSentence calculates outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Add smoothing to the model\n",
    "Already done, createBOWdataframe adds alpha for smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Fit different alphas; see accuracy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_alpha = list(np.round(np.linspace(0, 1, 11),6))\n",
    "\n",
    "arr_accuracy = []\n",
    "\n",
    "for i in arr_alpha:\n",
    "    arr_fresh_pred = myNBimplementation(i, df_train_V, df_test_V)\n",
    "    arr_fresh_true = df_test_V.fresh_yes.values\n",
    "    arr_accuracy.append(np.mean(arr_fresh_pred == arr_fresh_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    alpha       acc\n",
      "0     0.0  0.445580\n",
      "1     0.1  0.735309\n",
      "2     0.2  0.737353\n",
      "3     0.3  0.739397\n",
      "4     0.4  0.737353\n",
      "5     0.5  0.737353\n",
      "6     0.6  0.734798\n",
      "7     0.7  0.731221\n",
      "8     0.8  0.729177\n",
      "9     0.9  0.726622\n",
      "10    1.0  0.721513\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame({'alpha':arr_alpha, 'acc':arr_accuracy}))\n",
    "\n",
    "del arr_accuracy\n",
    "del arr_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Implement k-fold CV\n",
    "\n",
    "## 6.2 Find optimal alpha value with 5 fold CV\n",
    "Note- Ommitting alpha = 0 (still in table) because it's hard to see the trend when increasing the visible accuracy range to include 0.4 from the average of 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>acc</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452574</td>\n",
       "      <td>0.010067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.745604</td>\n",
       "      <td>0.007458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.751022</td>\n",
       "      <td>0.009083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.752248</td>\n",
       "      <td>0.008744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.752554</td>\n",
       "      <td>0.011298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.752145</td>\n",
       "      <td>0.010283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.750510</td>\n",
       "      <td>0.009982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.749488</td>\n",
       "      <td>0.011030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.747649</td>\n",
       "      <td>0.011142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.744991</td>\n",
       "      <td>0.010922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.741619</td>\n",
       "      <td>0.012126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alpha       acc       std\n",
       "0     0.0  0.452574  0.010067\n",
       "1     0.1  0.745604  0.007458\n",
       "2     0.2  0.751022  0.009083\n",
       "3     0.3  0.752248  0.008744\n",
       "4     0.4  0.752554  0.011298\n",
       "5     0.5  0.752145  0.010283\n",
       "6     0.6  0.750510  0.009982\n",
       "7     0.7  0.749488  0.011030\n",
       "8     0.8  0.747649  0.011142\n",
       "9     0.9  0.744991  0.010922\n",
       "10    1.0  0.741619  0.012126"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=5\n",
    "\n",
    "#Puts into random order\n",
    "df_train_K = df_train.sample(random_state = 1, n=len(df_train)).reset_index(drop=True)\n",
    "\n",
    "#Split into k-parts\n",
    "arr_df_train = np.array_split(df_train_K, k)\n",
    "\n",
    "#alpha arr\n",
    "arr_alpha_full = (np.round(np.linspace(0, 1, 11),6))\n",
    "\n",
    "#acc arr\n",
    "arr_accuracy = []\n",
    "arr_std = []\n",
    "\n",
    "#for alpha from 0-1\n",
    "for alpha in arr_alpha_full:\n",
    "\n",
    "    arr_acc_temp = []        \n",
    "\n",
    "    #for k times:\n",
    "    for i in range(k):\n",
    "\n",
    "        #Remove first element to test\n",
    "        dfKFOLD_test = arr_df_train[0].copy()\n",
    "        \n",
    "        #Delete test\n",
    "        del arr_df_train[0]\n",
    "\n",
    "        #set first element to train\n",
    "        dfKFOLD_train = arr_df_train[0].copy()\n",
    "        \n",
    "        #Join all corresponding dfs into train\n",
    "        for c in range(k - 2):\n",
    "            dfKFOLD_train = dfKFOLD_train.append(arr_df_train[c + 1])\n",
    "        \n",
    "        #calculate accuracy; add to temp accuracy array\n",
    "        arr_fresh_pred = myNBimplementation(alpha, dfKFOLD_train, dfKFOLD_test)\n",
    "        arr_fresh_true = dfKFOLD_test.fresh_yes.values\n",
    "        arr_acc_temp.append(np.mean(arr_fresh_pred == arr_fresh_true))        \n",
    "        \n",
    "        #Add test to end of df array\n",
    "        arr_df_train.append(dfKFOLD_test)\n",
    "\n",
    "    #add avg to accuracy array\n",
    "    arr_accuracy.append(np.mean(arr_acc_temp))\n",
    "    arr_std.append(np.std(arr_acc_temp))\n",
    "    \n",
    "df_thresh_acc = pd.DataFrame({'alpha':arr_alpha_full, 'acc':arr_accuracy, 'std':arr_std})\n",
    "df_thresh_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df_thresh_acc[1:11].alpha, df_thresh_acc[1:11].acc)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Alpha value')\n",
    "plt.title('Accuracy vs. Alpha level for NB over review data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Final model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"C:\\\\Users\\\\Arihan Jalan\\\\INFO 371\\\\rotten-tomatoes-test.csv\", sep='\\t')\n",
    "df_test = addFreshClass(cleanReviews(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7620173364854216\n",
      "Confusion Matrix: \n",
      " [[ 706  268]\n",
      " [ 336 1228]]\n"
     ]
    }
   ],
   "source": [
    "arr_fresh_pred = myNBimplementation(0.4, df_train, df_test)\n",
    "arr_fresh_true = df_test.fresh_yes\n",
    "\n",
    "print(\"accuracy:\", np.mean(arr_fresh_pred == arr_fresh_true))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(arr_fresh_true, arr_fresh_pred))\n",
    "\n",
    "del arr_fresh_pred\n",
    "del arr_fresh_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best kNN had an accuracy of 69.26%, and TF-IDF of 60.00%. Naive Bayes with it's accuracy of 76.20% is better than both of them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
