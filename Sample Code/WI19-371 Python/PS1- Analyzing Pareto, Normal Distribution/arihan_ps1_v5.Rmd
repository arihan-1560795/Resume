---
title: "PS1- Stats, Algebra"
output: html_document
---

## 1. Finding outliers in different distributions
Conduct Monte-Carlo simulations and infer results. Analyze distribution of means against sample selection, and sample size.

### 1.1 Normal Distribution
```{r}
#Functions for matrixes used by Normal Distributions in Part 1.1

#Returns a matrix with R rows of N data entries; the data is randomly, uniformly generated between 0 and 1
makeMatrix <- function(R, N){
  return(matrix(runif((R*N), min=0, max=1), nrow = R, ncol = N))
}

#Returns the per-row mean for a given 2-Dimensional matrix
matrixMean <- function(RN_matrix){
  return(rowMeans(RN_matrix, na.rm = TRUE))
}

#Prints the mean, variance, and sd for a 1-D numeric data structure assumed to be per-row means of a 2-D matrix
matrixMeanSummary <- function(RN_row_mean, R, N, distType){
  cat("Results for a matrix with row: ", R, ", col: ", N, ", filled with data from an ", distType, " simulation \n")
  cat("Mean of row means: ", mean(RN_row_mean),"\n")
  cat("Variance of row means: ", var(RN_row_mean),"\n")
  cat("Standard deviation of row means: ", sd(RN_row_mean),"\n")
}

#For the provided 1-D numeric data structure, prints values corresponding to the provided upper and lower bounds.
matrixMeanCI <- function(RN_row_mean, lower, upper){
  cat("The row means are bounded by: ", quantile(RN_row_mean, probs = c(lower, upper))," for a ", upper," CI \n")
}
```

<br/>

#### 1.1.1. Pick your number of repetitions R (1000 is a good choice), and sample sizes N (10, 1000, 100,000) are good choices, but you may want to adjust if the latter is too slow)

<br/>

#### 1.1.2. For R times create sample of N of standard normals, and take the mean of it. You'll have R means of N standard normals.
```{r}
kxk_matrix <- makeMatrix(1000,1000)
kxk_row_mean <- matrixMean(kxk_matrix)
```

<br/>

#### 1.1.3. Compute the mean-of-means, it's standard deviation, and 95% confidence region.
```{r}
#   The 95% confidence region is the opposite to computing the percentage of observations that fit into
#   the interval. The easiest way to and it is using sample quantiles. Check out the quantile function
#   in R (or np.percentile) function in python. For instance, quantile(x, 10) will give the 10th
#   percentile q10 of a vector x. 10th percentile is such a value that 10% of the sample values are smaller
#   than that value. See OS 1.6.5 (p 35) for some more explanations.
matrixMeanSummary(kxk_row_mean, 1000, 1000, "MC")

#   95% CI = mean - qnorm of (5%/2)*(sd/sqrt(n))
matrixMeanCI(kxk_row_mean, 0.05, 0.95)
```

<br/> 

#### 1.1.4. What do you expect the confidence region to be based on the theoretical considerations?
The values generated by runif are uniformly distributed, which when averages should be close to 0.5 We calculated the means for a thousand of such instances, and then took the resulting means 95% CI. When plotting a histogram of the means, they're almost normally distributed around 0.47 and 0.53. Given both of this, I expected the CI to be close to 0.5, and the results from the quantiles method support this.

<br/>

#### 1.1.5 5. Repeat the above with three different (and I mean very different) N-s, such as 10, 1000, 100,000. Show how the mean-of-means, it's standard deviation, and the confidence region depends on N
```{r}
# N = 10
kxt_matrix <- makeMatrix(1000, 10)
kxt_row_mean <- matrixMean(kxt_matrix)
matrixMeanSummary(kxt_row_mean, 1000, 10, "MC")
matrixMeanCI(kxt_row_mean, 0.05, 0.95)

# N = 100
kxh_matrix <- makeMatrix(1000, 100)
kxh_row_mean <- matrixMean(kxh_matrix)
matrixMeanSummary(kxh_row_mean, 1000, 100, "MC")
matrixMeanCI(kxh_row_mean, 0.05, 0.95)

# N = 1000
kxKK_matrix <- makeMatrix(1000, 1000)
kxKK_row_mean <- matrixMean(kxKK_matrix)
matrixMeanSummary(kxKK_row_mean, 1000, 100000, "MC")
matrixMeanCI(kxKK_row_mean, 0.05, 0.95)
```
As the value of N increases, the row means move closer to 0.5, the variance and sd decrease, the confidence intervals become closer bound.

<br/>

### 1.2 Pareto Distributions
```{r}
#Functions for matrixes used by Pareto Distributions in Part 1.2

#Takes the following values and returns a Pareto distribution of values
#   n = # of values, 
#   location = where located/ centered around (default = 1)
#   shape = k = shape/inequality (large = less inequality))
makePareto <- function(n, location, shape){
  return(VGAM::rpareto(n, location, shape))
}

#Makes a Pareto Matrix of R*N values centered around location distributed by shape
#Where R is the number of rows and N number of columns
makeParetoMatrix <- function(R, N, location, shape){
  return(matrix(makePareto(R*N, location, shape), nrow = R, ncol = N))
}

#Takes a pareto distribution and plots it logaritmitically if logBool is TRUE
#Also plots a red line along the mean 
plotPareto <- function(pareto_x, logBool){
  if (logBool == TRUE){
    h <- hist(log10(pareto_x), xaxt = "n")
    abline(v=log10(mean(pareto_x)),  col = "red")
    axis(1, at = pretty(h$breaks), labels = 10^pretty(h$breaks))
  } else {
    hist(pareto_x)
    abline(v=(mean(pareto_x)),  col = "red")
  }
}

```

<br/>

#### 1.2.1 Pick three different k-values; and plot their histograms.
```{r}
pareto_1 <- makePareto(10000, 1, 1)
plotPareto(pareto_1, TRUE)

pareto_10 <- makePareto(10000, 1, 10)
plotPareto(pareto_10, FALSE)

pareto_100 <- makePareto(10000, 1, 100)
plotPareto(pareto_100, FALSE)
```

#### 1.2.2 Comment on the shape of the histograms
As the shape or inequality value increases, the variance in the generated numbers decreases. This is apparent by generated values being spread across a smaller range, with the mean being closer to the selected location 1.

<br/>

#### 1.2.3. pick two k values. One of these should be less than 1, the other greater than 1. 0.5 and 2 are good choices. Now, for each value of k, repeat the same task as you did above with random normals (pick #repetitions R, sample size N; calculate mean, mean-of-means, sd, 95% CI)
```{r}
for(k_value in c(0.5, 2)){
  for(n_value in c(10, 10000)){
    cat("k value: ", k_value, "\n")
    pMatrix <- makeParetoMatrix(1000, n_value, 1, k_value)
    pMatrix_mean <- matrixMean(pMatrix)
    matrixMeanSummary(pMatrix_mean, 1000, n_value, "Pareto")
    matrixMeanCI(pMatrix_mean, 0.05, 0.95)
    plotPareto(pMatrix_mean, FALSE)
    cat("\n")
  }
}
```

<br/>

#### 1.2.4. Finally compare your results for normal and Pareto distributions: how does the confidence region depend on n in case of the normal distribution? How in case of pareto distribution? And how does the latter depend on k?
Normally distributed data benefits from having a larger N- with numbers bounded and uniformly distributed between 0 and 1, as the amount of numbers to sample from increases, we know inherently that the mean will lie closer to 0.5, and that can be confirmed with a greater degree of confidence, as suggested by the shrinking variance and SD which suggests the data is more tightly distributed and the shrinking confidence intervals with which we can say we are 95% sure that the true mean lies within the set bounds.
    
There are 2 cases of increasing N with Pareto distributions, one with a small, and large k.

The smaller a k-value, the greater spread out are numbers between 1 and infinity. With a small k and large N, this means a greater likelihood of seeing outliers in our data, which drastically pushes our mean away from 1, and decreases the the certainity of us knowing where the true mean lies as is with the increasing confidence intervals as N increases.

As k-increases though, our generated numbers lie closer to 1; and we see fewer outliers. As the data is more tightly distributed, our confidence interval benefits from this; as N increases, so our CI decreases, and we can say with greater certainity where the true mean lies.
