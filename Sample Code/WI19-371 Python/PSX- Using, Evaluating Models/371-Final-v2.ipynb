{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from scipy import stats # ANOVA\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn import *\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Load Boston Housing Dataset\n",
    "Note- The data is numeric; I printed X and there is no non-numeric data in it; it's also free of MEDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Methods to predict MEDV \n",
    "## Cover method name, str, weakness, and hyperparams\n",
    "## Note- methods like SVM are NOT meant for predicting data, but rather classifying it. Also, LogisticRegression is ONLY for classifying data in Python; LinearRegression has no paramaters that can be tweaked from sklearn\n",
    "\n",
    ".\n",
    "\n",
    "### Linear Regression\n",
    "Models an output based on a linear combination of input data.\n",
    "#### Advantages\n",
    "* Works in high dimensional data (many columns)\n",
    "* Works fast, and has results/params that are easy to understand\n",
    "#### Disadvantages\n",
    "* Is sensitive to outliers\n",
    "* Non-linear relationships require feature-engineering to fix\n",
    "#### Hyperparameter\n",
    "* (for logistic) C for regularization strength- lower C = more underfitting\n",
    "* (for logistic) method- what scalar algorithm to use\n",
    "* (theoretical; sklearn doesn't allow tuning this for prediction data)- tolerance/ease of it- how much will the algorithm will weigh points near/across the regression line\n",
    "\n",
    ".\n",
    "\n",
    "### kNN\n",
    "For a given set of testing data; it looks at which points in the training set are the closest match to it and assigns the target for the training point one with the closest match.\n",
    "#### Advantages\n",
    "* It's fast to set up; no training cost (instance based learning)\n",
    "* Easy to understand results\n",
    "* Flexible; can deal with overfitting by changing K\n",
    "#### Disadvantages\n",
    "* Does not work in high dimensional spaces\n",
    "#### Hyperparameter\n",
    "* k- The numbers of neighbours to consider. For instance- if k=1- only the nearest point is considered; if k=5- 5 neighbours are considered and the ones which appear the most are considered.\n",
    "\n",
    ".\n",
    "\n",
    "### Decision Trees\n",
    "Splits the data into categories based on a feature value to try and maximize information gain.\n",
    "#### Advantages\n",
    "* easy to understand results\n",
    "* uses decision making; can become arbitrary complexity\n",
    "* can be made compact through pruning\n",
    "#### Disadvantages\n",
    "* sometimes hard to interpret\n",
    "#### Hyperparameter\n",
    "* (for classification) critereon- function to measure how to split\n",
    "* max_features- number of features to consider when looking for the best split\n",
    "\n",
    ".\n",
    "\n",
    "### Regularization (Ridge/Lasso)\n",
    "Adding/Subtracting features and weighing them on how well they help predict the target feature. Assign weights/drop feature based on how well they help predict.\n",
    "#### Advantages\n",
    "* Lasso removes variables that have little to no correlation/impact on our outcome variable; wheras Ridge should bring it close to zero- both help regularization scale across high dimensional data by considering features that help predict the target feature. \n",
    "* Lasso removes can remove features and reduce our model complexity and computational cost\n",
    "#### Disadvantages\n",
    "* Models sometimes hard to interpret\n",
    "#### Hyperparameter\n",
    "* Alpha- multiplies the L1 term (tbh- not sure on what it does very well) for Lasso, Ridge.\n",
    "\n",
    ".\n",
    "\n",
    "## SVM can work (it has something for regression and predicting continuous data), but it's best suited for classification for categorical data than regression for continous data and will hence not be considered for the sake of this exercise.\n",
    "\n",
    "### SVM\n",
    "#### Advantages\n",
    "* Is more flexible than Logistic/Linear Regression\n",
    "* Kernels engineer features automtically\n",
    "* Describes complex decision boundaries well\n",
    "* Only cases than violate the margin have influence- which can be adjusted by budgeting\n",
    "* Works well in non-seperable cases unlike classifiers that try and maximize the margins\n",
    "#### Disadvantages\n",
    "* Doesn't work very well in high dimensional spaces\n",
    "* It's still inherently a linear classifier\n",
    "#### Hyperparameter\n",
    "* Kernels- What SVM algorithm to use and classify the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Predict MEDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalized data Xn\n",
    "scaler = preprocessing.StandardScaler()\n",
    "Xn = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#r2 taken from https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "#df sorting https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html\n",
    "\n",
    "#Arrays to store data for each test\n",
    "\n",
    "#model name\n",
    "model = []\n",
    "\n",
    "#parameter used for the given model\n",
    "param = []\n",
    "\n",
    "#whether the data used is normalized or not\n",
    "normalized = []\n",
    "\n",
    "#r2 and rmse scores\n",
    "r2 = []\n",
    "mse = []\n",
    "\n",
    "xArr = [X, Xn]\n",
    "xArrPrint = [0, 1]\n",
    "\n",
    "#for normalized, count data\n",
    "    #for different features for a given model\n",
    "        #calculate avg rmse, mse for k=v cross val\n",
    "        #append to array\n",
    "    \n",
    "for i in range(len(xArr)):\n",
    "    #Lasso\n",
    "    for iAlpha in list(np.round(np.linspace(0, 1, 6),6)):\n",
    "        r2.append(np.mean(cross_val_score(Lasso(alpha=iAlpha), X=xArr[i], y=y, scoring='r2', cv=5)))\n",
    "        mse.append(np.mean(cross_val_score(Lasso(alpha=iAlpha), X=xArr[i], y=y, scoring='neg_mean_squared_error', cv=5)))\n",
    "        normalized.append(xArrPrint[i])\n",
    "        param.append(\"alpha:\" + str(np.round(iAlpha, 5)))\n",
    "        model.append(\"Lasso\")\n",
    "\n",
    "    #Ridge\n",
    "    for iAlpha in list(np.round(np.linspace(0, 1, 6),6)):\n",
    "        r2.append(np.mean(cross_val_score(Ridge(alpha=iAlpha), X=xArr[i], y=y, scoring='r2', cv=5)))\n",
    "        mse.append(np.mean(cross_val_score(Ridge(alpha=iAlpha), X=xArr[i], y=y, scoring='neg_mean_squared_error', cv=5)))\n",
    "        normalized.append(xArrPrint[i])\n",
    "        param.append(\"alpha:\" + str(np.round(iAlpha, 5)))\n",
    "        model.append(\"Ridge\")\n",
    "\n",
    "    #kNN\n",
    "    for iKnn in range(1,10):\n",
    "        r2.append(np.mean(cross_val_score(KNeighborsRegressor(n_neighbors = iKnn), X=xArr[i], y=y, scoring='r2', cv=5)))\n",
    "        mse.append(np.mean(cross_val_score(KNeighborsRegressor(n_neighbors = iKnn), X=xArr[i], y=y, scoring='neg_mean_squared_error', cv=5)))\n",
    "        normalized.append(xArrPrint[i])\n",
    "        param.append(\"# of Neighbours: \" + str(iKnn))\n",
    "        model.append(\"kNN\")\n",
    "\n",
    "    #Decision Trees\n",
    "    for iFeature in ['sqrt', 'log2', 'auto']:\n",
    "        r2.append(np.mean(cross_val_score(DecisionTreeRegressor(max_features = iFeature), X=xArr[i], y=y, scoring='r2', cv=5)))\n",
    "        mse.append(np.mean(cross_val_score(DecisionTreeRegressor(max_features = iFeature), X=xArr[i], y=y, scoring='neg_mean_squared_error', cv=5)))\n",
    "        normalized.append(xArrPrint[i])\n",
    "        param.append(\"Feature: \" + iFeature)\n",
    "        model.append(\"Decision Tree\")\n",
    "\n",
    "    #Linear Regression\n",
    "    r2.append(np.mean(cross_val_score(LinearRegression(), X=xArr[i], y=y, scoring='r2', cv=5)))\n",
    "    mse.append(np.mean(cross_val_score(LinearRegression(), X=xArr[i], y=y, scoring='neg_mean_squared_error', cv=5)))\n",
    "    normalized.append(xArrPrint[i])\n",
    "    param.append(\"N/A\")\n",
    "    model.append(\"LogisticRegression\")\n",
    "\n",
    "mse = [ np.sqrt(-x) for x in mse]\n",
    "dfBostonModels = pd.DataFrame({'classifier':model, 'parameter':param, 'normalized':normalized, 'r2':r2, 'rmse':mse})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>parameter</th>\n",
       "      <th>normalized</th>\n",
       "      <th>r2</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>kNN</td>\n",
       "      <td># of Neighbours: 9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.540138</td>\n",
       "      <td>5.323236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>kNN</td>\n",
       "      <td># of Neighbours: 8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.527135</td>\n",
       "      <td>5.370247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>kNN</td>\n",
       "      <td># of Neighbours: 7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.511286</td>\n",
       "      <td>5.402913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>kNN</td>\n",
       "      <td># of Neighbours: 6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.503050</td>\n",
       "      <td>5.425458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>kNN</td>\n",
       "      <td># of Neighbours: 5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.493679</td>\n",
       "      <td>5.436011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>kNN</td>\n",
       "      <td># of Neighbours: 4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.491846</td>\n",
       "      <td>5.492649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>kNN</td>\n",
       "      <td># of Neighbours: 3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.457022</td>\n",
       "      <td>5.604815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>alpha:0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.434296</td>\n",
       "      <td>5.932281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>alpha:1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.431849</td>\n",
       "      <td>5.960837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>alpha:0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.431360</td>\n",
       "      <td>5.910723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>alpha:0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.426917</td>\n",
       "      <td>5.870644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>Feature: log2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.415843</td>\n",
       "      <td>9.164978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>alpha:0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.410545</td>\n",
       "      <td>5.883390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>alpha:0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.409502</td>\n",
       "      <td>5.970584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>alpha:1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.389218</td>\n",
       "      <td>5.938580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>alpha:0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.385831</td>\n",
       "      <td>5.952313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>alpha:0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.381452</td>\n",
       "      <td>5.970282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>alpha:0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.381314</td>\n",
       "      <td>6.110052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>kNN</td>\n",
       "      <td># of Neighbours: 2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.376341</td>\n",
       "      <td>5.789833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>alpha:0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.375505</td>\n",
       "      <td>5.995160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>alpha:0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.366897</td>\n",
       "      <td>6.032254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>alpha:0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.366232</td>\n",
       "      <td>6.177178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>alpha:1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.360686</td>\n",
       "      <td>6.069792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>alpha:0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.359257</td>\n",
       "      <td>6.074407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>alpha:0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.357803</td>\n",
       "      <td>6.079093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>alpha:0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.356322</td>\n",
       "      <td>6.083850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>alpha:0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.354813</td>\n",
       "      <td>6.088681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>alpha:0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.353276</td>\n",
       "      <td>6.093587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>alpha:0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.353276</td>\n",
       "      <td>6.093587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>alpha:0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.353276</td>\n",
       "      <td>6.093587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1</td>\n",
       "      <td>0.353276</td>\n",
       "      <td>6.093587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>alpha:0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.353276</td>\n",
       "      <td>6.093587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>0.353276</td>\n",
       "      <td>6.093587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>alpha:0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.347349</td>\n",
       "      <td>6.265588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>alpha:1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.321081</td>\n",
       "      <td>6.372272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>Feature: sqrt</td>\n",
       "      <td>0</td>\n",
       "      <td>0.151848</td>\n",
       "      <td>7.851180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>Feature: auto</td>\n",
       "      <td>0</td>\n",
       "      <td>0.110836</td>\n",
       "      <td>6.491817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>Feature: auto</td>\n",
       "      <td>1</td>\n",
       "      <td>0.043698</td>\n",
       "      <td>6.584733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>Feature: sqrt</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.022477</td>\n",
       "      <td>6.666685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>Feature: log2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.064330</td>\n",
       "      <td>8.199605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>kNN</td>\n",
       "      <td># of Neighbours: 1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.151519</td>\n",
       "      <td>7.068317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>kNN</td>\n",
       "      <td># of Neighbours: 9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.274295</td>\n",
       "      <td>8.634958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>kNN</td>\n",
       "      <td># of Neighbours: 8</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.289360</td>\n",
       "      <td>8.667203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>kNN</td>\n",
       "      <td># of Neighbours: 7</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.305335</td>\n",
       "      <td>8.710694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>kNN</td>\n",
       "      <td># of Neighbours: 5</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.315016</td>\n",
       "      <td>8.771715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>kNN</td>\n",
       "      <td># of Neighbours: 4</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.320685</td>\n",
       "      <td>8.763755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>kNN</td>\n",
       "      <td># of Neighbours: 6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.325678</td>\n",
       "      <td>8.725794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>kNN</td>\n",
       "      <td># of Neighbours: 3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.380546</td>\n",
       "      <td>8.871437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>kNN</td>\n",
       "      <td># of Neighbours: 2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.472561</td>\n",
       "      <td>9.258515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>kNN</td>\n",
       "      <td># of Neighbours: 1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.027172</td>\n",
       "      <td>10.289524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            classifier           parameter  normalized        r2       rmse\n",
       "45                 kNN  # of Neighbours: 9           1  0.540138   5.323236\n",
       "44                 kNN  # of Neighbours: 8           1  0.527135   5.370247\n",
       "43                 kNN  # of Neighbours: 7           1  0.511286   5.402913\n",
       "42                 kNN  # of Neighbours: 6           1  0.503050   5.425458\n",
       "41                 kNN  # of Neighbours: 5           1  0.493679   5.436011\n",
       "40                 kNN  # of Neighbours: 4           1  0.491846   5.492649\n",
       "39                 kNN  # of Neighbours: 3           1  0.457022   5.604815\n",
       "4                Lasso           alpha:0.8           0  0.434296   5.932281\n",
       "5                Lasso           alpha:1.0           0  0.431849   5.960837\n",
       "3                Lasso           alpha:0.6           0  0.431360   5.910723\n",
       "2                Lasso           alpha:0.4           0  0.426917   5.870644\n",
       "22       Decision Tree       Feature: log2           0  0.415843   9.164978\n",
       "1                Lasso           alpha:0.2           0  0.410545   5.883390\n",
       "26               Lasso           alpha:0.2           1  0.409502   5.970584\n",
       "11               Ridge           alpha:1.0           0  0.389218   5.938580\n",
       "10               Ridge           alpha:0.8           0  0.385831   5.952313\n",
       "9                Ridge           alpha:0.6           0  0.381452   5.970282\n",
       "27               Lasso           alpha:0.4           1  0.381314   6.110052\n",
       "38                 kNN  # of Neighbours: 2           1  0.376341   5.789833\n",
       "8                Ridge           alpha:0.4           0  0.375505   5.995160\n",
       "7                Ridge           alpha:0.2           0  0.366897   6.032254\n",
       "28               Lasso           alpha:0.6           1  0.366232   6.177178\n",
       "36               Ridge           alpha:1.0           1  0.360686   6.069792\n",
       "35               Ridge           alpha:0.8           1  0.359257   6.074407\n",
       "34               Ridge           alpha:0.6           1  0.357803   6.079093\n",
       "33               Ridge           alpha:0.4           1  0.356322   6.083850\n",
       "32               Ridge           alpha:0.2           1  0.354813   6.088681\n",
       "25               Lasso           alpha:0.0           1  0.353276   6.093587\n",
       "0                Lasso           alpha:0.0           0  0.353276   6.093587\n",
       "6                Ridge           alpha:0.0           0  0.353276   6.093587\n",
       "49  LogisticRegression                 N/A           1  0.353276   6.093587\n",
       "31               Ridge           alpha:0.0           1  0.353276   6.093587\n",
       "24  LogisticRegression                 N/A           0  0.353276   6.093587\n",
       "29               Lasso           alpha:0.8           1  0.347349   6.265588\n",
       "30               Lasso           alpha:1.0           1  0.321081   6.372272\n",
       "21       Decision Tree       Feature: sqrt           0  0.151848   7.851180\n",
       "23       Decision Tree       Feature: auto           0  0.110836   6.491817\n",
       "48       Decision Tree       Feature: auto           1  0.043698   6.584733\n",
       "46       Decision Tree       Feature: sqrt           1 -0.022477   6.666685\n",
       "47       Decision Tree       Feature: log2           1 -0.064330   8.199605\n",
       "37                 kNN  # of Neighbours: 1           1 -0.151519   7.068317\n",
       "20                 kNN  # of Neighbours: 9           0 -0.274295   8.634958\n",
       "19                 kNN  # of Neighbours: 8           0 -0.289360   8.667203\n",
       "18                 kNN  # of Neighbours: 7           0 -0.305335   8.710694\n",
       "16                 kNN  # of Neighbours: 5           0 -0.315016   8.771715\n",
       "15                 kNN  # of Neighbours: 4           0 -0.320685   8.763755\n",
       "17                 kNN  # of Neighbours: 6           0 -0.325678   8.725794\n",
       "14                 kNN  # of Neighbours: 3           0 -0.380546   8.871437\n",
       "13                 kNN  # of Neighbours: 2           0 -0.472561   9.258515\n",
       "12                 kNN  # of Neighbours: 1           0 -1.027172  10.289524"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#metrics.SCORERS.keys()\n",
    "dfBostonModels.sort_values(by=['r2'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Discussion\n",
    "* r-squared, or r2 measures the proportion of variance that is explained by the independent variables; with higher being better. It can be interpreted as how close the data is to the fitted model.\n",
    "\n",
    "* rmse is a number that is the root mean square error; it's the squared differences between predicted to actual values for a model that then have their square root taken. Lower is better- with 0 being no difference between predicted and actual values (perfect prediction)\n",
    "\n",
    "The kNN model with normalized data dominated- having the highest r-squared and lowest rmse value among all the models. If I had to take a blind guess as to why it performed the best- it'd probably be because housing generally follows some trends- for instance; housing that's nearer a lake or water body is generally more expensive than those that are landlocked; those with higher crime rates generally have lower prices than those that are not; or those that are newer are more expensive because of newer features like broader roads and updated utilities. Given this, a newly established town that is close to a river is likely to have housing values similar or close to those that are also newly established and close to a river. Given that similar neighbourhoods will probably have similar prices, kNN is likely to perform well here.\n",
    "\n",
    "kNN performs well because of the nature of this dataset- in that properties that share features are likely to be priced similarly. It's also likely given the dataset had many (13) features, some of the other models like decision trees and regression overfitted on the data which caused them to perform badly.\n",
    "\n",
    "To try and explain why normalized data performed really well for kNN, I found this explanation on Stackoverflow that really hit the mark-\n",
    "https://stats.stackexchange.com/questions/287425/why-do-you-need-to-scale-data-in-knn\n",
    "\n",
    "> Suppose you had a dataset (m \"examples\" by n \"features\") and all but one feature dimension had values strictly between 0 and 1, while a single feature dimension had values that range from -1000000 to 1000000. When taking the euclidean distance between pairs of \"examples\", the values of the feature dimensions that range between 0 and 1 may become uninformative and the algorithm would essentially rely on the single dimension whose values are substantially larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[\n",
    "# [[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[\n",
    "# [[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[\n",
    "# [[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[\n",
    "# [[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in dataset\n",
    "\n",
    "#https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Load dataset; split into train/test/validation\n",
    "## Note- Imported data from sklearn; so results may be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set X, Y\n",
    "#https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.html\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "#https://github.com/javedsha/text-classification/blob/master/Text%2BClassification%2Busing%2Bpython%2C%2Bscikit%2Band%2Bnltk.ipynb\n",
    "#Convert to count\n",
    "count_vect = CountVectorizer()\n",
    "X_c = count_vect.fit_transform(X)\n",
    "\n",
    "#Convert to TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_t = tfidf_transformer.fit_transform(X_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Convert text into BOW; find best k for kNN through CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "kNN parameters comparison:\n",
      "    k: 1 Time: 5.24 Acc: 0.64\n",
      "    k: 2 Time: 5.2 Acc: 0.6\n",
      "    k: 3 Time: 5.33 Acc: 0.58\n",
      "    k: 4 Time: 5.57 Acc: 0.56\n",
      "    k: 5 Time: 5.6 Acc: 0.54\n",
      "    k: 6 Time: 6.01 Acc: 0.53\n",
      "    k: 7 Time: 6.85 Acc: 0.53\n",
      "    k: 8 Time: 6.56 Acc: 0.52\n",
      "    k: 9 Time: 6.31 Acc: 0.51\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_c, y, random_state = 1, stratify = y, test_size = 0.2)\n",
    "\n",
    "print(\"\")\n",
    "print(\"kNN parameters comparison:\")\n",
    "for i in range(1,10):\n",
    "    start_time = time.time()\n",
    "    knnModel = KNeighborsClassifier(n_neighbors = i, metric='cosine')\n",
    "    acc = cross_val_score(knnModel, X_train, y_train, cv=5)\n",
    "    print(\"    k:\", i, \"Time:\",str(round(time.time() - start_time,2)), \"Acc:\", np.round(  np.mean(acc)  , 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neighbors.VALID_METRICS['brute']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Repeat 2.2 with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "kNN parameters comparison:\n",
      "    k: 1 Time: 5.0 Acc: 0.8\n",
      "    k: 2 Time: 5.04 Acc: 0.77\n",
      "    k: 3 Time: 5.14 Acc: 0.77\n",
      "    k: 4 Time: 5.3 Acc: 0.76\n",
      "    k: 5 Time: 5.44 Acc: 0.76\n",
      "    k: 6 Time: 6.51 Acc: 0.76\n",
      "    k: 7 Time: 6.49 Acc: 0.75\n",
      "    k: 8 Time: 6.28 Acc: 0.75\n",
      "    k: 9 Time: 6.11 Acc: 0.75\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_t, y, random_state = 1, stratify = y, test_size = 0.2)\n",
    "\n",
    "print(\"\")\n",
    "print(\"kNN parameters comparison:\")\n",
    "for i in range(1,10):\n",
    "    start_time = time.time()\n",
    "    knnModel = KNeighborsClassifier(n_neighbors = i, metric='cosine')\n",
    "    acc = cross_val_score(knnModel, X_train, y_train, cv=5)\n",
    "    print(\"    k:\", i, \"Time:\",str(round(time.time() - start_time,2)), \"Acc:\", np.round(  np.mean(acc)  , 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Rpeat 2.2 with NB; finding best smoothing param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BOW Count NB smoothing comparison:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    alpha: 0.0 Time: 0.68 Acc: 0.87\n",
      "    alpha: 0.1 Time: 0.71 Acc: 0.87\n",
      "    alpha: 0.2 Time: 0.63 Acc: 0.87\n",
      "    alpha: 0.3 Time: 0.73 Acc: 0.86\n",
      "    alpha: 0.4 Time: 0.73 Acc: 0.85\n",
      "    alpha: 0.5 Time: 0.73 Acc: 0.85\n",
      "    alpha: 0.6 Time: 0.77 Acc: 0.84\n",
      "    alpha: 0.7 Time: 0.74 Acc: 0.83\n",
      "    alpha: 0.8 Time: 0.76 Acc: 0.83\n",
      "    alpha: 0.9 Time: 0.76 Acc: 0.82\n",
      "    alpha: 1.0 Time: 0.74 Acc: 0.82\n",
      "\n",
      "TF-IDF NB smoothing comparison:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    alpha: 0.0 Time: 0.72 Acc: 0.87\n",
      "    alpha: 0.1 Time: 0.71 Acc: 0.89\n",
      "    alpha: 0.2 Time: 0.77 Acc: 0.88\n",
      "    alpha: 0.3 Time: 0.81 Acc: 0.87\n",
      "    alpha: 0.4 Time: 0.76 Acc: 0.86\n",
      "    alpha: 0.5 Time: 0.78 Acc: 0.85\n",
      "    alpha: 0.6 Time: 0.73 Acc: 0.85\n",
      "    alpha: 0.7 Time: 0.7 Acc: 0.84\n",
      "    alpha: 0.8 Time: 0.73 Acc: 0.84\n",
      "    alpha: 0.9 Time: 0.74 Acc: 0.83\n",
      "    alpha: 1.0 Time: 0.72 Acc: 0.83\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_c, y, random_state = 1, stratify = y, test_size = 0.2)\n",
    "\n",
    "print(\"\")\n",
    "print(\"BOW Count NB smoothing comparison:\")\n",
    "for i in list(np.round(np.linspace(0, 1, 11),6)):\n",
    "    start_time = time.time()\n",
    "    nb = MultinomialNB(alpha = i)\n",
    "    acc = cross_val_score(nb, X_train, y_train, cv=5)\n",
    "    print(\"    alpha:\", i, \"Time:\",str(round(time.time() - start_time,2)), \"Acc:\", np.round(  np.mean(acc)  , 2))\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_t, y, random_state = 1, stratify = y, test_size = 0.2)\n",
    "\n",
    "print(\"\")\n",
    "print(\"TF-IDF NB smoothing comparison:\")\n",
    "for i in list(np.round(np.linspace(0, 1, 11),6)):\n",
    "    start_time = time.time()\n",
    "    nb = MultinomialNB(alpha = i)\n",
    "    acc = cross_val_score(nb, X_train, y_train, cv=5)\n",
    "    print(\"    alpha:\", i, \"Time:\",str(round(time.time() - start_time,2)), \"Acc:\", np.round(  np.mean(acc)  , 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Discuss Findings\n",
    "Naive Bayes took less than a 5th of the time to train and test than kNN did; and had a higher accuracy all round. Based on our training-validation tests; we can argue Naive Bayes better models our data than kNN does given this higher accuracy at a lower computational time.  kNN doesn't perform as well because there are too many words; aka- too many dimensions. Even mostly similar sentences are still really far away because a few words may not match or be contained in another sentence. \n",
    "\n",
    "Within Naive Bayes, we had the highest accuracy after converting our BOW to TF-IDF and choosing 0.1 as our smoothing parameter. For the last step, we will use naive bayes with alpha=0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 Analyze model on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    alpha: 0.1 Acc: 0.9\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_t, y, random_state = 1, stratify = y, test_size = 0.2)\n",
    "\n",
    "nb = MultinomialNB(alpha = 0.1).fit(X = X_train, y = y_train)\n",
    "prediction = nb.predict(X_test)\n",
    "print(\"    alpha:\", 0.1, \"Acc:\", np.round(  (prediction == y_test).sum()/len(prediction)  , 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
