{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "stop = stopwords.words('english')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Loading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1- LOAD THE DATA\n",
    "texts = pd.read_csv(\"D:\\\\OneDrive\\\\University of Washington Seattle\\\\Year 4 Quarter 2 (Winter)\\\\INFO 371\\\\Problem Set\\\\PS4\\\\texts.csv\", sep='\\t')\n",
    "texts.chunk = texts['chunk'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Inspecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of file: (12924, 5)\n",
      "\n",
      "Description of file: \n",
      " int64     3\n",
      "object    2\n",
      "dtype: int64\n",
      "\n",
      "Head of file: \n",
      "                               name    size  lines  chunkid  \\\n",
      "0  balbulus-early-life-charlemagne  259062   4394        1   \n",
      "1  balbulus-early-life-charlemagne  259062   4394        2   \n",
      "2  balbulus-early-life-charlemagne  259062   4394        3   \n",
      "3  balbulus-early-life-charlemagne  259062   4394        4   \n",
      "4  balbulus-early-life-charlemagne  259062   4394        5   \n",
      "\n",
      "                                               chunk  \n",
      "0  \\ntitle: early lives of charlemagne by eginhar...  \n",
      "1  \\n\\nthe notes, keyed to line numbers in the so...  \n",
      "2  \\n         from a bronze statuette in the mus√©...  \n",
      "3  \\n                _a lui finit la dissolution ...  \n",
      "4  public opinion in regard to the meaning of fal...  \n"
     ]
    }
   ],
   "source": [
    "#1.2- Inspect some of the texts. Note that chunkid 1 corresponds to the first page of the text.\n",
    "print('Shape of file:', texts.shape)\n",
    "print(\"\")\n",
    "print('Description of file: \\n', texts.dtypes.value_counts())\n",
    "print(\"\")\n",
    "print(\"Head of file: \\n\", texts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Unique Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique texts in data: \n",
      "\n",
      "balbulus-early-life-charlemagne\n",
      "beesly-queen-elizabeth\n",
      "bible\n",
      "carroll-alice-wonderland\n",
      "chipman-earliest-electromagnetic-instruments\n",
      "cia-world-factbook-1992\n",
      "eckstein-quintus-claudius\n",
      "fisher-quaker-colonies\n",
      "gallienne-quest-of-golden-girl\n",
      "gordon-quiet-talks-crowned-christ\n",
      "hardy-madding-crowd\n",
      "infiltrating-open-systems\n",
      "kant-metaphysical-elements-ethics\n",
      "karn-snowflakes\n",
      "milton-paradise-lost\n",
      "naval-academy-sound-military-decision\n",
      "newsgroup\n",
      "paper-compact-hash-tables\n",
      "paper-data-compression\n",
      "paper-logical-implementation-of-arithmetic\n",
      "paper-programming-by-example\n",
      "paper-search-for-autonomy\n",
      "selected-polish-tales\n",
      "shakespeare-as-you-like-it\n",
      "unamuno-tragic-sense-of-life\n",
      "vaneeden-quest\n",
      "webster-early-european-history\n",
      "why-speech-output\n",
      "workshop-proceedings\n"
     ]
    }
   ],
   "source": [
    "#1.3- List all the text sources listed in variable name\n",
    "\n",
    "#Return only unique values in dataframe\n",
    "unique_texts = texts.name.unique()\n",
    "\n",
    "#Print unique values using for loop for formatting\n",
    "print(\"Unique texts in data: \\n\")\n",
    "for x in unique_texts: \n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.1 Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processes a given string\n",
    "# * replaces all whitespace with a single splace\n",
    "# * removes all non-English alphabets (numbers, special charachters, punctuation)\n",
    "# * Removes stop words (e.g.- the, a, in, an, etc.)\n",
    "def change_me(content):\n",
    "    content = str(content)\n",
    "    content = re.sub(r\"[^A-Z ]+\", \"\", content, 0, re.IGNORECASE)\n",
    "    content = re.sub(r\"[ ]{2,}\", \" \", content, 0, re.IGNORECASE)\n",
    "\n",
    "    word_tokens = word_tokenize(content)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_sentence = []\n",
    "\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "\n",
    "    filtered_sentence = [porter.stem(word) for word in filtered_sentence]\n",
    "            \n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "#Do this for all the string \"chunk\" variables in texts\n",
    "texts.chunk = texts.chunk.apply(change_me)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.5 Splitting data intro training, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3.5- Split data into testing, training\n",
    "\n",
    "#Samples the given dataframe for n random rows and returns training, testing, and validation data split on given number\n",
    "def sampleTheDf(df, nbr, testSplit, valSplit):\n",
    "    sTexts = df.dropna().sample(n = nbr, random_state = 1)\n",
    "    \n",
    "    #Break data into training, testing sets\n",
    "    train, test = train_test_split(sTexts, train_size = testSplit, random_state = 1)\n",
    "    train_t, val_t = train_test_split(train, train_size = valSplit, random_state = 1)\n",
    "\n",
    "    #Reset index\n",
    "    train = train.reset_index()\n",
    "    test = test.reset_index()\n",
    "\n",
    "    train_t = train_t.reset_index()\n",
    "    val_t = val_t.reset_index()\n",
    "    \n",
    "    #To return all the data, putting them into a dictionary\n",
    "    dataTable = {\n",
    "      \"train\": train,\n",
    "      \"test\": test,\n",
    "      \"train_t\": train_t,\n",
    "      \"val_t\": val_t\n",
    "    }\n",
    "    return dataTable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Creating Dictionary of words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.4- Create the Dictionary\n",
    "def createDict(train, test):\n",
    "    \n",
    "    # merged training and validation data\n",
    "    sentences = np.concatenate((train.chunk.values, test.chunk.values), axis=0)\n",
    "\n",
    "    ## initialize the voctorizer\n",
    "    vectorizer = CountVectorizer(min_df=0)\n",
    "\n",
    "    ## create the dictionary\n",
    "    # `fit` builds the vocabulary\n",
    "    vectorizer.fit(sentences)\n",
    "\n",
    "    ## transform your data into the BOW array\n",
    "    # rows are sentences, columns are words\n",
    "    X = vectorizer.transform(sentences).toarray()\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Function to find cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.5- Find cosine similarity between two vectors\n",
    "\n",
    "#The cosine similarity = dot product / (length (v1) * length (v2))\n",
    "#Returns number between 0 and 1 which tells us how similar the two vectors are\n",
    "def cosine_sim(a, b):\n",
    "\n",
    "    #Find the dot product# 1.4 Creating Dictionary of words (BOW)\n",
    "    dot_ab = np.dot(a, b)\n",
    "    \n",
    "    #Find the length/magnite of vectors\n",
    "    mag_a = np.linalg.norm(a)\n",
    "    mag_b = np.linalg.norm(b)\n",
    "\n",
    "    #Return cosine similarity\n",
    "    return (dot_ab / (mag_a * mag_b))\n",
    "\n",
    "#The dot product of 2 vectors is-\n",
    "#a = 1, 2, 3, 4\n",
    "#b = 1, 2, 3, 4\n",
    "#a.b = 1*1 + 2*2 + 3*3 + 4*4\n",
    "\n",
    "#The magnitude of a vector \n",
    "#a = 1, 2, 3, 4\n",
    "#|a| = sqrt(12 + 22 + 32 + 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6, 1.7 Implement kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.6 Implementing KNN\n",
    "def kNNSolver(k, X_train_mtx, Y_val, train_t):\n",
    "\n",
    "    #m_score keeps track of cosine scores\n",
    "    m_score = []\n",
    "    \n",
    "    #Find cosine for each value in training data\n",
    "    for point in X_train_mtx:\n",
    "        m_score.append(cosine_sim(point, Y_val))\n",
    "\n",
    "    #Find top k values based on cosine score\n",
    "    #Create df linking cosine scores to names\n",
    "    a = pd.DataFrame.from_dict({'score': m_score, 'book': train_t.name})\n",
    "\n",
    "    #Select the largest k elements\n",
    "    a = a.nlargest(k, 'score')\n",
    "\n",
    "    #Select the most common value from the subset of k elements\n",
    "    a = a.groupby(['book']).agg(lambda x:x.value_counts().index[0])\n",
    "    a = a.reset_index()\n",
    "    a = a.book[0]\n",
    "\n",
    "    return a\n",
    "\n",
    "#returns dataframe of actual vs. predicted values\n",
    "def kNN(k, X, train_t, val_t):\n",
    "    \n",
    "    #Create dataframe of actual vs. predicted names\n",
    "    testCheck = val_t.name.reset_index()\n",
    "\n",
    "    #Stores predicted values from KNN; added as col. to testCheck\n",
    "    predictArray = []\n",
    "\n",
    "    #Split X into train, test data\n",
    "    \n",
    "    #X_train_mtx = subset of matrix X to training data\n",
    "    X_train_mtx = X[0:len(train_t)]\n",
    "    \n",
    "    #Y_val_mtx = subset of matrix X to validation or test data\n",
    "    Y_val_mtx = X[len(train_t):len(train_t) + len(val_t)]\n",
    "\n",
    "    #For each value in our testing data\n",
    "    for Y_val in Y_val_mtx:\n",
    "        val_P = kNNSolver(k, X_train_mtx, Y_val, train_t)\n",
    "        predictArray.append(val_P)\n",
    "\n",
    "    testCheck['predicted'] = predictArray\n",
    "    return(testCheck[['name', 'predicted']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.8, 1.9 Testing implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to memory and time limitations, only choosing 3000 samples\n",
    "dataTable = sampleTheDf(texts, 3000, 0.9, 0.9) \n",
    "\n",
    "#Set up train, test based on return\n",
    "train = dataTable.get('train')\n",
    "test = dataTable.get('test')\n",
    "train_t = dataTable.get('train_t')\n",
    "val_t = dataTable.get('val_t')\n",
    "X = createDict(train_t, val_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data size: 2430\n",
      "Testing Data size: 270\n",
      "\n",
      "k-value: 1\n",
      "Accuracy: 0.7925925925925926\n",
      "Time taken: 91.97316336631775\n",
      "\n",
      "k-value: 5\n",
      "Accuracy: 0.6259259259259259\n",
      "Time taken: 100.10655927658081\n",
      "\n",
      "k-value: 10\n",
      "Accuracy: 0.5259259259259259\n",
      "Time taken: 94.31973338127136\n",
      "\n",
      "k-value: 20\n",
      "Accuracy: 0.42962962962962964\n",
      "Time taken: 93.92343378067017\n",
      "\n",
      "k-value: 25\n",
      "Accuracy: 0.40370370370370373\n",
      "Time taken: 98.62302422523499\n"
     ]
    }
   ],
   "source": [
    "#Describing Data set sample\n",
    "print(\"Training Data size:\", len(train_t))\n",
    "print(\"Testing Data size:\", len(val_t))\n",
    "\n",
    "timeArray = []\n",
    "accArray = []\n",
    "\n",
    "#Testing different versions of k\n",
    "for k in [1, 5, 10, 20, 25]:\n",
    "    start_time = time.time()\n",
    "    c = kNN(k, X, train_t, val_t)\n",
    "    c = len(c[c.name == c.predicted]) / len(c)\n",
    "    timeTaken = time.time()-start_time\n",
    "\n",
    "    timeArray.append(timeTaken)\n",
    "    accArray.append(c)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"k-value:\", k)\n",
    "    print(\"Accuracy:\", c)\n",
    "    print(\"Time taken:\", timeTaken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using kNN over our training and validation data suggests the highest accuracy is found with a k-value of 1 with an accuracy of 79.26%. This value also had the best performance, indicated by it taking the least time to compute.\n",
      "\n",
      "The following results are from using these findings over our training and testing data:\n",
      "\n",
      "Training Data size: 2700\n",
      "Testing Data size: 300\n",
      "\n",
      "Time taken: 129.74847412109375\n",
      "Accuracy: 0.7666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Using kNN over our training and validation data suggests the highest accuracy is found with a k-value of 1 with an accuracy of 79.26%. This value also had the best performance, indicated by it taking the least time to compute.\")\n",
    "print(\"\")\n",
    "print(\"The following results are from using these findings over our training and testing data:\")\n",
    "print(\"\")\n",
    "\n",
    "#Train, Test already defined\n",
    "X = createDict(train, test)\n",
    "print(\"Training Data size:\", len(train))\n",
    "print(\"Testing Data size:\", len(test))\n",
    "print(\"\")\n",
    "\n",
    "#Testing data\n",
    "start_time = time.time()\n",
    "c = kNN(1, X, train, test)\n",
    "c = len(c[c.name == c.predicted]) / len(c)\n",
    "print(\"Time taken:\", time.time()-start_time)\n",
    "print(\"Accuracy:\", c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Create TF-IDF transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates TF, IDF, and returns TF-IDF matrix for data\n",
    "def createTFIDFmatrix(X):\n",
    "    X_TF = X/X.sum(axis=1)[:,None]\n",
    "    X_IDF = np.log(X/X.sum(axis=0) + 1)\n",
    "    X_TFIDF = np.multiply(X_TF,X_IDF)\n",
    "    return X_TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Test TF-IDF transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between same vector: 1.0\n",
      "\n",
      "Cosine similarity between diff vector w/o TF-IDF: 0.02952\n",
      "Cosine similarity between diff vector w TF-IDF: 2e-05\n"
     ]
    }
   ],
   "source": [
    "X_TFIDF = createTFIDFmatrix(X)\n",
    "print(\"Cosine similarity between same vector:\", cosine_sim(X_TFIDF[1], X_TFIDF[1]))\n",
    "print(\"\")\n",
    "print(\"Cosine similarity between diff vector w/o TF-IDF:\", np.round_(cosine_sim(X[1], X[2]), decimals = 5))\n",
    "print(\"Cosine similarity between diff vector w TF-IDF:\", np.round_(cosine_sim(X_TFIDF[1], X_TFIDF[2]), decimals = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Testing TF-IDF on kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data size: 2430\n",
      "Testing Data size: 270\n",
      "\n",
      "k-value: 1\n",
      "Accuracy: 0.6925925925925925\n",
      "Time taken: 24.836672067642212\n",
      "\n",
      "k-value: 5\n",
      "Accuracy: 0.4962962962962963\n",
      "Time taken: 26.776031017303467\n",
      "\n",
      "k-value: 10\n",
      "Accuracy: 0.34444444444444444\n",
      "Time taken: 31.796534299850464\n",
      "\n",
      "k-value: 20\n",
      "Accuracy: 0.29259259259259257\n",
      "Time taken: 30.60167169570923\n",
      "\n",
      "k-value: 25\n",
      "Accuracy: 0.27037037037037037\n",
      "Time taken: 31.457271814346313\n"
     ]
    }
   ],
   "source": [
    "X = createDict(train_t, val_t)\n",
    "X = createTFIDFmatrix(X)\n",
    "\n",
    "#Describing Data set sample\n",
    "print(\"Training Data size:\", len(train_t))\n",
    "print(\"Testing Data size:\", len(val_t))\n",
    "\n",
    "timeArray = []\n",
    "accArray = []\n",
    "\n",
    "#Testing different versions of k\n",
    "for k in [1, 5, 10, 20, 25]:\n",
    "    start_time = time.time()\n",
    "    c = kNN(k, X, train_t, val_t)\n",
    "    c = len(c[c.name == c.predicted]) / len(c)\n",
    "    timeTaken = time.time()-start_time\n",
    "\n",
    "    timeArray.append(timeTaken)\n",
    "    accArray.append(c)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"k-value:\", k)\n",
    "    print(\"Accuracy:\", c)\n",
    "    print(\"Time taken:\", timeTaken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Interpreting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF is MUCH faster, but less accurate than had the transformation not been applied. In general, as k increases, accuracy decreases. I think this may be the case because I've already removed all the stop words and cleaned the data quite a bit. Had those stop words still been in there, the results may have been different.\n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF is MUCH faster, but less accurate than had the transformation not been applied. In general, as k increases, accuracy decreases. I think this may be the case because I've already removed all the stop words and cleaned the data quite a bit. Had those stop words still been in there, the results may have been different.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I forgot about the numbering/ question numbers, so we're pretending that didn't happen. ;) \n",
    "\n",
    "# 2.1 Inspecting Rotten Tomatoes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of file: (13442, 9)\n",
      "\n",
      "Description of file: \n",
      " object    7\n",
      "int64     2\n",
      "dtype: int64\n",
      "\n",
      "Head of file: \n",
      "                critic  fresh    imdb  \\\n",
      "0         Derek Adams  fresh  114709   \n",
      "1     Richard Corliss  fresh  114709   \n",
      "2         David Ansen  fresh  114709   \n",
      "3       Leonard Klady  fresh  114709   \n",
      "4  Jonathan Rosenbaum  fresh  114709   \n",
      "\n",
      "                                                link     publication  \\\n",
      "0  http://www.timeout.com/film/reviews/87745/toy-...        Time Out   \n",
      "1  http://www.time.com/time/magazine/article/0,91...   TIME Magazine   \n",
      "2                  http://www.newsweek.com/id/104199        Newsweek   \n",
      "3  http://www.variety.com/review/VE1117941294.htm...         Variety   \n",
      "4  http://onfilm.chicagoreader.com/movies/capsule...  Chicago Reader   \n",
      "\n",
      "                                               quote          review_date  \\\n",
      "0  So ingenious in concept, design and execution ...  2009-10-04 00:00:00   \n",
      "1                  The year's most inventive comedy.  2008-08-31 00:00:00   \n",
      "2  A winning animated feature that has something ...  2008-08-18 00:00:00   \n",
      "3  The film sports a provocative and appealing st...  2008-06-09 00:00:00   \n",
      "4  An entertaining computer-generated, hyperreali...  2008-03-10 00:00:00   \n",
      "\n",
      "   rtid      title  \n",
      "0  9559  Toy Story  \n",
      "1  9559  Toy Story  \n",
      "2  9559  Toy Story  \n",
      "3  9559  Toy Story  \n",
      "4  9559  Toy Story  \n"
     ]
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"D:\\\\OneDrive\\\\University of Washington Seattle\\\\Year 4 Quarter 2 (Winter)\\\\INFO 371\\\\Problem Set\\\\PS4\\\\reviews.csv\", sep=',')\n",
    "\n",
    "print('Shape of file:', reviews.shape)\n",
    "print(\"\")\n",
    "print('Description of file: \\n', reviews.dtypes.value_counts())\n",
    "print(\"\")\n",
    "print(\"Head of file: \\n\", reviews.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing cases where fresh isn't assigned and quotes are empty\n",
    "reviews = reviews[(reviews.fresh != 'none') & (reviews.quote != '') & (reviews.quote != None)]\n",
    "\n",
    "#Removing duplicates by grouping by all columns\n",
    "reviews = pd.DataFrame(reviews.groupby(list(reviews)).size().reset_index())\n",
    "\n",
    "#Cleaning data by removing stop words, punctuation, numbers, etc. from quotes\n",
    "reviews.quote = reviews.quote.str.lower()\n",
    "reviews.quote = reviews.quote.apply(change_me)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3, 2.4 Test over data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to memory and time limitations, only choosing 3000 samples\n",
    "dataTable = sampleTheDf(reviews, 3000, 0.9, 0.9) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.1 create dictionary and BOW of all quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODIFYING KNN TO BE USABLE OVER NEW DATA (chunk -> quote)\n",
    "#1.4- Create the Dictionary\n",
    "def createDict(train, test):\n",
    "    \n",
    "    # merged training and validation data\n",
    "    sentences = np.concatenate((train.quote.values, test.quote.values), axis=0)\n",
    "\n",
    "    ## initialize the voctorizer\n",
    "    vectorizer = CountVectorizer(min_df=0)\n",
    "\n",
    "    ## create the dictionary\n",
    "    # `fit` builds the vocabulary\n",
    "    vectorizer.fit(sentences)\n",
    "\n",
    "    ## transform your data into the BOW array\n",
    "    # rows are sentences, columns are words\n",
    "    X = vectorizer.transform(sentences).toarray()\n",
    "\n",
    "    return X\n",
    "\n",
    "#Set up train, test based on return\n",
    "train = dataTable.get('train')\n",
    "test = dataTable.get('test')\n",
    "train_t = dataTable.get('train_t')\n",
    "val_t = dataTable.get('val_t')\n",
    "X = createDict(train_t, val_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.2 test fresh/rotten over different k-values; compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODIFYING KNN TO BE USABLE OVER NEW DATA (name -> fresh) (chunk -> quote) (name -> actual)\n",
    "#1.6 Implementing KNN\n",
    "def kNNSolver(k, X_train_mtx, Y_val, train_t):\n",
    "\n",
    "    #m_score keeps track of cosine scores\n",
    "    m_score = []\n",
    "    \n",
    "    #Find cosine for each value in training data\n",
    "    for point in X_train_mtx:\n",
    "        m_score.append(cosine_sim(point, Y_val))\n",
    "\n",
    "    #Find top k values based on cosine score\n",
    "    #Create df linking cosine scores to names\n",
    "    a = pd.DataFrame.from_dict({'score': m_score, 'fresh': train_t.fresh})\n",
    "\n",
    "    #Select the largest k elements\n",
    "    a = a.nlargest(k, 'score')\n",
    "\n",
    "    #Select the most common value from the subset of k elements\n",
    "    a = a.groupby(['fresh']).agg(lambda x:x.value_counts().index[0])\n",
    "    a = a.reset_index()\n",
    "    a = a.fresh[0]\n",
    "\n",
    "    return a\n",
    "\n",
    "#returns dataframe of actual vs. predicted values\n",
    "def kNN(k, X, train_t, val_t):\n",
    "    \n",
    "    #Create dataframe of actual vs. predicted names\n",
    "    testCheck = val_t.fresh.reset_index()\n",
    "\n",
    "    #Stores predicted values from KNN; added as col. to testCheck\n",
    "    predictArray = []\n",
    "\n",
    "    #Split X into train, test data\n",
    "    \n",
    "    #X_train_mtx = subset of matrix X to training data\n",
    "    X_train_mtx = X[0:len(train_t)]\n",
    "    \n",
    "    #Y_val_mtx = subset of matrix X to validation or test data\n",
    "    Y_val_mtx = X[len(train_t):len(train_t) + len(val_t)]\n",
    "\n",
    "    #For each value in our testing data\n",
    "    for Y_val in Y_val_mtx:\n",
    "        val_P = kNNSolver(k, X_train_mtx, Y_val, train_t)\n",
    "        predictArray.append(val_P)\n",
    "\n",
    "    testCheck['predicted'] = predictArray\n",
    "    return(testCheck[['fresh', 'predicted']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data size: 2430\n",
      "Testing Data size: 270\n",
      "\n",
      "k-value: 1\n",
      "Accuracy: 0.562962962962963\n",
      "Time taken: 27.854827642440796\n",
      "\n",
      "k-value: 5\n",
      "Accuracy: 0.5407407407407407\n",
      "Time taken: 30.965736627578735\n",
      "\n",
      "k-value: 10\n",
      "Accuracy: 0.5370370370370371\n",
      "Time taken: 30.986567735671997\n",
      "\n",
      "k-value: 20\n",
      "Accuracy: 0.5370370370370371\n",
      "Time taken: 31.330294370651245\n",
      "\n",
      "k-value: 25\n",
      "Accuracy: 0.5370370370370371\n",
      "Time taken: 32.60551118850708\n"
     ]
    }
   ],
   "source": [
    "#Describing Data set sample\n",
    "print(\"Training Data size:\", len(train_t))\n",
    "print(\"Testing Data size:\", len(val_t))\n",
    "\n",
    "timeArray = []\n",
    "accArray = []\n",
    "\n",
    "#Testing different versions of k\n",
    "for k in [1, 5, 10, 20, 25]:\n",
    "    start_time = time.time()\n",
    "    c = kNN(k, X, train_t, val_t)\n",
    "    c = len(c[c.fresh == c.predicted]) / len(c)\n",
    "    timeTaken = time.time()-start_time\n",
    "\n",
    "    timeArray.append(timeTaken)\n",
    "    accArray.append(c)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"k-value:\", k)\n",
    "    print(\"Accuracy:\", c)\n",
    "    print(\"Time taken:\", timeTaken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.3 Transform your data into TF-IDF form and repeat k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data size: 2430\n",
      "Testing Data size: 270\n",
      "\n",
      "k-value: 1\n",
      "Accuracy: 0.6\n",
      "Time taken: 10.979442834854126\n",
      "\n",
      "k-value: 5\n",
      "Accuracy: 0.5518518518518518\n",
      "Time taken: 14.063071489334106\n",
      "\n",
      "k-value: 10\n",
      "Accuracy: 0.5370370370370371\n",
      "Time taken: 14.14788556098938\n",
      "\n",
      "k-value: 20\n",
      "Accuracy: 0.5370370370370371\n",
      "Time taken: 13.709917545318604\n",
      "\n",
      "k-value: 25\n",
      "Accuracy: 0.5370370370370371\n",
      "Time taken: 13.917245149612427\n"
     ]
    }
   ],
   "source": [
    "X = createDict(train_t, val_t)\n",
    "X = createTFIDFmatrix(X)\n",
    "\n",
    "#Describing Data set sample\n",
    "print(\"Training Data size:\", len(train_t))\n",
    "print(\"Testing Data size:\", len(val_t))\n",
    "\n",
    "timeArray = []\n",
    "accArray = []\n",
    "\n",
    "#Testing different versions of k\n",
    "for k in [1, 5, 10, 20, 25]:\n",
    "    start_time = time.time()\n",
    "    c = kNN(k, X, train_t, val_t)\n",
    "    c = len(c[c.fresh == c.predicted]) / len(c)\n",
    "    timeTaken = time.time()-start_time\n",
    "\n",
    "    timeArray.append(timeTaken)\n",
    "    accArray.append(c)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"k-value:\", k)\n",
    "    print(\"Accuracy:\", c)\n",
    "    print(\"Time taken:\", timeTaken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.4 Inspect, explain cases where tomato was correctly/incorrectly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data size: 2430\n",
      "Testing Data size: 270\n"
     ]
    }
   ],
   "source": [
    "#MODIFYING KNN TO BE USABLE OVER NEW DATA (name -> fresh) (chunk -> quote) (name -> actual)\n",
    "#1.6 Implementing KNN\n",
    "def kNNSolver(k, X_train_mtx, Y_val, train_t):\n",
    "\n",
    "    #m_score keeps track of cosine scores\n",
    "    m_score = []\n",
    "    \n",
    "    #Find cosine for each value in training data\n",
    "    for point in X_train_mtx:\n",
    "        m_score.append(cosine_sim(point, Y_val))\n",
    "\n",
    "    #Find top k values based on cosine score\n",
    "    #Create df linking cosine scores to names\n",
    "    a = pd.DataFrame.from_dict({'score': m_score, 'fresh': train_t.fresh})\n",
    "\n",
    "    #Select the largest k elements\n",
    "    a = a.nlargest(k, 'score')\n",
    "    a = a.reset_index()\n",
    "\n",
    "    a = a.groupby(['fresh']).agg(lambda x:x.value_counts().index[0])\n",
    "    a = a.reset_index()\n",
    "\n",
    "#    print(a.fresh[0])\n",
    "#    print(a.score[0])\n",
    "#    print(a)\n",
    "\n",
    "    a = {\n",
    "      \"fresh\": a.fresh[0],\n",
    "      \"score\": a.score[0],\n",
    "    }\n",
    "    return(a)\n",
    "    \n",
    "\n",
    "\n",
    "#returns dataframe of actual vs. predicted values\n",
    "def kNN(k, X, train_t, val_t):\n",
    "    \n",
    "    #Create dataframe of actual vs. predicted names\n",
    "    testCheck = val_t[['fresh', 'quote']].reset_index()\n",
    "    \n",
    "    #Stores predicted values from KNN; added as col. to testCheck\n",
    "    predictArray = []\n",
    "    predictScore = []\n",
    "\n",
    "    #Split X into train, test data\n",
    "    \n",
    "    #X_train_mtx = subset of matrix X to training data\n",
    "    X_train_mtx = X[0:len(train_t)]\n",
    "    \n",
    "    #Y_val_mtx = subset of matrix X to validation or test data\n",
    "    Y_val_mtx = X[len(train_t):len(train_t) + len(val_t)]\n",
    "\n",
    "    #For each value in our testing data\n",
    "    for Y_val in Y_val_mtx:\n",
    "        val_P = kNNSolver(k, X_train_mtx, Y_val, train_t)\n",
    "        predictArray.append(val_P.get('fresh'))\n",
    "        predictScore.append(val_P.get('score'))\n",
    "    testCheck['predicted'] = predictArray\n",
    "    testCheck['score'] = predictScore\n",
    "    return(testCheck)\n",
    "\n",
    "X = createDict(train_t, val_t)\n",
    "X = createTFIDFmatrix(X)\n",
    "\n",
    "#Describing Data set sample\n",
    "print(\"Training Data size:\", len(train_t))\n",
    "print(\"Testing Data size:\", len(val_t))\n",
    "\n",
    "timeArray = []\n",
    "accArray = []\n",
    "\n",
    "#Testing different versions of k\n",
    "c = kNN(1, X, train_t, val_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                           quote   fresh\n",
      "185                                                    worth see   fresh\n",
      "107                                        troubl film need want   fresh\n",
      "152                                                    fun watch   fresh\n",
      "116  wisdom put bad experi behind us never rang true view mov...  rotten\n",
      "215  mani dickey lumpi narr idea remain screenplay john boorm...  rotten\n"
     ]
    }
   ],
   "source": [
    "cFalse = c[c.fresh != c.predicted]\n",
    "pd.options.display.max_colwidth = 60\n",
    "print(cFalse.sort_values(by=['score'], ascending=False)[['quote', 'fresh']].head())\n",
    "#pd.options.display.max_colwidth = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit hard to tell why the above quotes are false given they've been modified to the point where they're unrecognizable, but if I had to guess- it is probably because they used a negative word like \"no\" or \"trouble\" or had been sarcastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Summary\n",
    "\n",
    "The pages worked better than the reviews with a higher accuraccy of 79% vs 60%. \n",
    "\n",
    "TF-IDF worked better than BOW (difference = 4% for k=1) for the review dataset, but \n",
    "BOW worked better than TF-IDF (difference = 9% for k=1) for the pages dataset.\n",
    "Across both datasets, TF-IDF completed about 3 times faster than BOW.\n",
    "\n",
    "All 4 cases had their highest accuracy at k=1, with it dropping as k increased."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
